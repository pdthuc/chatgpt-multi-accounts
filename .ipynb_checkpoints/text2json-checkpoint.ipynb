{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster to valid, prefixOnly, empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 1000:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "#                             if search_string in content:\n",
    "                            if content.count(search_string) == 5:\n",
    "                                prefixOnly_files.append(file_path)\n",
    "                            else:\n",
    "                                valid_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "\n",
    "# def txt2json(list_of_file_names):\n",
    "#     incorrect_list = []\n",
    "#     for filename in list_of_file_names:\n",
    "#         try:\n",
    "#             with open(f'{filename}', 'r', encoding='utf-8') as file:\n",
    "#                 content = file.read()\n",
    "#             content = content.replace(\"json\\nCopy code\\n\", \"\").replace('''\"{''', \"{\").replace('''}\"''', \"}\")\n",
    "#             json_object = json.loads(content, strict=False)\n",
    "#             new_filename = filename.replace('.txt', '.json').replace(directory, 'test')\n",
    "#             with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "#                 json.dump(json_object, json_file, ensure_ascii=False, indent=4)\n",
    "#         except:\n",
    "#             incorrect_list.append(filename)\n",
    "#     return incorrect_list\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# directory_to_search = 'instructions_social_1k_post_khang_prompt'\n",
    "# search_string = 'Dựa vào ý của task'\n",
    "\n",
    "# result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "# # fail_files = txt2json(result_dict['valid'])\n",
    "\n",
    "# print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "# Valid : {len(result_dict['valid']) - len(fail_files)}\n",
    "# Fail: {len(fail_files)}\n",
    "# PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "# Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can NOT found json format.\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import re\n",
    "\n",
    "# def extract_json_from_text(text):\n",
    "#     match = re.search(r'\\{.*\\}', text)\n",
    "\n",
    "#     if match:\n",
    "#         json_string = match.group()\n",
    "#         try:\n",
    "#             json_data = json.loads(json_string)\n",
    "#             return json_data\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"Error decoding JSON: {e}\")\n",
    "#             return None\n",
    "#     else:\n",
    "#         print(\"Can NOT found json format.\")\n",
    "#         return None\n",
    "\n",
    "    \n",
    "# path = fail_files[0]\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     content = f.read()\n",
    "# result = extract_json_from_text(content)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if sub_str in string\n",
    "# Total: 7065\n",
    "# Valid : 4044\n",
    "# Fail: 248\n",
    "# PrefixOnly: 1447\n",
    "# Empty:1326\n",
    "    \n",
    "    \n",
    "# if string contain 5 sub_str(s)\n",
    "# Total: 7071\n",
    "# Valid : 4105\n",
    "# Fail: 287\n",
    "# PrefixOnly: 1352\n",
    "# Empty:1327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_nth(haystack: str, needle: str, n: int, reverse: bool = False) -> int:\n",
    "#     if not reverse:\n",
    "#         start = haystack.find(needle)\n",
    "#         while start >= 0 and n > 1:\n",
    "#             start = haystack.find(needle, start + len(needle))\n",
    "#             n -= 1\n",
    "#     else:\n",
    "#         start = haystack.rfind(needle)\n",
    "#         while start >= 0 and n > 1:\n",
    "#             start = haystack.rfind(needle, 0, start)\n",
    "#             n -= 1\n",
    "#     return start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefix Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {\n",
    "    1: \"Phân tích và tóm tắt nội dung\",\n",
    "    2: \"Trích xuất thông tin với các thuộc tính có thể có trong bài\",\n",
    "    3: \"Sinh ra tiêu đề\",\n",
    "    4: \"Tạo tên chủ đề\",\n",
    "    5: \"Tạo 5 bình luận có thể có\",\n",
    "    6: \"Phân loại nội dung\",\n",
    "    7: \"Phát hiện các thực thể\",\n",
    "    8: \"Trích xuất thông tin với các thuộc tính có thể có trong bài\",\n",
    "    9: \"Tìm sản phẩm có thể liên quan\",\n",
    "    10: \"Tạo đánh giá\",\n",
    "    11: \"Tạo 2 câu hỏi và câu trả lời tương ứng\",\n",
    "    12: \"Sinh ra bình luận ý định mua hàng\",\n",
    "    13: \"Phân loại thực thể\",\n",
    "    14: \"Dự đoán nhóm khách hàng tiềm năng\",\n",
    "    15: \"Phát hiện giá trị thuộc tính\",\n",
    "    16: \"Trích xuất thông tin người mua hoặc bán\",\n",
    "    17: \"Sinh ra bài viết tương tự với sắc thái khác\",\n",
    "    18: \"Sinh ra thuộc tính có thể trích xuất phù hợp\",\n",
    "    19: \"Sửa lỗi chính tả và viết tắt\",\n",
    "    20: \"Phân tích các emoji\",\n",
    "    21: \"Phân tích thái độ cảm xúc\",\n",
    "    22: \"Sinh ra nội dung với sắc thái cảm xúc khác\",\n",
    "    23: \"Sinh ra ý tưởng quảng cáo sản phẩm\",\n",
    "    24: \"Sinh ra câu hỏi thăm dò ý kiến khách hàng\",\n",
    "    25: \"Sinh 1 FAQ cho sản phẩm\",\n",
    "    26: \"Tạo 5 bình luận có thể có sắc thái cảm xúc khác\",\n",
    "    27: \"Tạo 5 hashtag cho bài viết\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "list_of_file_names = result_dict['prefixOnly']\n",
    "incorrect_prefixOnly_list = []\n",
    "for filename in list_of_file_names:\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        content = content.replace(\"json\\nCopy code\\n\", \"\").replace('''\"{''', \"{\").replace('''}\"''', \"}\")\n",
    "        number_pattern = re.compile(r'\\d+')\n",
    "\n",
    "        json_object = json.loads(content, strict=False)\n",
    "        for key, value in json_object['output'].items():\n",
    "            search_text = key[:2] + key[-2:]\n",
    "            match = number_pattern.search(search_text)\n",
    "            if match:\n",
    "                matching_key = int(match.group())\n",
    "                append_text = task_dict[matching_key]\n",
    "            else:\n",
    "                append_text = key\n",
    "                matching_key = next((key for key, value in task_dict.items() if value == append_text), None)\n",
    "\n",
    "            value[\"augmented_instruction\"] = f\"{value['augmented_instruction']}. {append_text}\"\n",
    "\n",
    "        new_filename = filename.replace('.txt', '.json').replace('generated_data_10k_follow_format_task', 'json_10k_prefix_only')\n",
    "        \n",
    "        with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(json_object, json_file, ensure_ascii=False, indent=4)\n",
    "    except:\n",
    "        incorrect_prefixOnly_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1776, 150)"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_file_names),len(incorrect_prefixOnly_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chuẩn hoá key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invalid_list=[]\n",
    "# # directory = \"json_generated_data_10k_follow_format_task\"\n",
    "# directory = \"json_10k_prefix_only\"\n",
    "# number_pattern = re.compile(r'\\d+')\n",
    "\n",
    "# for root, dirs, files in os.walk(directory):\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(root, file)\n",
    "#         if os.path.isfile(file_path):\n",
    "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                 d = json.load(f)\n",
    "#             try:\n",
    "#                 new_dict = {}\n",
    "#                 for key, value in d['output'].items():\n",
    "# #                     print(f\"***{file_path,key}***\")\n",
    "#                     search_text = key[:2] + key[-2:]\n",
    "#                     match = number_pattern.search(search_text)\n",
    "#                     if match:\n",
    "#                         matching_key = int(match.group())\n",
    "#                         append_text = task_dict[matching_key]\n",
    "#                     else:\n",
    "#                         append_text = key.replace('hastag', 'hashtag').replace('_', ' ')\n",
    "#                         matching_key = next((key for key, value in task_dict.items() if value == append_text), None)\n",
    "\n",
    "#                     new_dict[matching_key] = d['output'][key]\n",
    "#                 new_filename = file_path.replace(directory, 'standard_json_prefix_only_10k')\n",
    "#                 with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "#                     json.dump(new_dict, json_file, ensure_ascii=False, indent=4)\n",
    "#             except:\n",
    "#                 invalid_list.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5414 + 1626 == 7040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None Tao5Hashtag\n",
    "# None DuDoanNhomKhachHangTiemNang\n",
    "# None PhatHienCacThucThe\n",
    "# None Sinhh ra câu hỏi thăm dò ý kiến khách hàng\n",
    "# None Sinhh 1 FAQ cho sản phẩm\n",
    "# None Sinhr ra nội dung với sắc thái cảm xúc khác\n",
    "# None analyze_and_summarize_content\n",
    "# None extract_information_with_possible_attributes\n",
    "# None generate_5_possible_comments\n",
    "# None generate_appropriate_extracted_attributes\n",
    "# None classify_content\n",
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhFAQChoSanPham\n",
    "# None SinhCauHoiThamDoYKienKhachHang\n",
    "# None PhanLoaiNoiDung\n",
    "# None PhanTichTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhBaiVietTuongTuSaoThaiKhac\n",
    "# None SinhBinhLuanYDinhMuaHang\n",
    "# None SinhTieuDe\n",
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhFAQChoSanPham\n",
    "# None SinhRaTieuDe\n",
    "# None PhatHienGiaTriThuocTinh\n",
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None PhanLoaiThucThe\n",
    "# None TaoTenChuDe\n",
    "# None TrichXuatThongTinThuocTinh\n",
    "# None PhanTichTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhCauHoiTheoYKienKhachHang\n",
    "# None Tao2CauHoiVaCauTraLoiTuongUng\n",
    "# None SinhFAQChoSanPham\n",
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhFAQChoSanPham\n",
    "# None SinhRaTieuDe\n",
    "# None TrichXuatThongTinNguoiMuaBan\n",
    "# None analyze_and_summarize_content\n",
    "# None extract_information\n",
    "# None generate_faq_for_product\n",
    "# None find_related_products\n",
    "# None classify_content\n",
    "# None PhanTichTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhRaCauHoiThamDoYKienKhachHang\n",
    "# None PhanLoaiNoiDung\n",
    "# None PhanLoaiThucThe\n",
    "# None Sinhr ra bài viết tương tự với sắc thái khác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = {\n",
    "    \"sample_10k\": \"\"\"Bạn là một AI trợ lý tạo file JSON, nhiệm vụ của bạn chỉ là tạo JSON và bạn luôn luôn trả về JSON không có thêm mô tả nào khác. Hãy dựa vào ý của từng task trong task list tạo câu hỏi hay yêu cầu mới đa dạng, tự nhiên và có thể thực hiện đối với nội dung trong INPUT được cung cấp, sau đó tự trả lời.\n",
    "\n",
    "Task list:\n",
    "{tasks}\n",
    "\n",
    "INPUT:\n",
    "{post}\n",
    "\n",
    "Chú ý: Phải trả về dưới dạng một json file duy nhất theo format đảm bảo đầy đủ các fields trong đó, nếu instruction không phù hợp với đoạn văn thì trả lời với ý không có câu trả lời trong đoạn văn. Hãy chỉ trả lời một file json duy nhất, sau đây là định dạng json file cần tuân theo trong mọi trường hợp:\n",
    "{{\n",
    "\"task_name\": {{\n",
    "\"augmented_instruction\": \"Dựa vào ý của task sau đó tạo câu hỏi hoặc câu lệnh với tác vụ đã chọn, hãy tạo đa dạng từ ngữ và tự nhiên\",\n",
    "\"json_output\": <Trả lời với dạng JSON>,\n",
    "\"text_output\": <Trả lời với dạng text>\n",
    "}},\n",
    "\"task_name\": ...\n",
    "}}\n",
    "\n",
    "Chú ý: Hãy trả lời theo format json trên không thêm các phần bên ngoài, không được phân chia thành json nhỏ cho mỗi task. Hãy cố gắng trả lời và trả lời tuân theo format và luôn cố gắng trả lời đủ các tasks\"\"\",\n",
    "    \"khang_prompt_1k\" : \"\"\"Bạn là chuyên gia social listening phân tích toàn diện và đầy đủ cho bài đăng\n",
    "Các tác vụ gợi ý như sau: câu chủ đề, tóm tắt ngắn gọn ý chính, đánh giá tổng quan thái độ, trích xuất thông tin, liện hệ trực tiếp, liên hệ trực tuyến, trích xuất thực thể và các thuộc tính, trích xuất cảm xúc theo khía cạnh, phân tích ý nghĩa ẩn dụ các emoji, các highlight keywords.\n",
    "Lưu ý:  chỉ trả về json. cuối json phân tích cần đưa ra vài quan điểm trái chiều với bài và lời khuyên đối với người đọc. \n",
    "Bài Viết:\n",
    "{post}\n",
    "\"\"\",\n",
    "    'bao_prompt_2k':  \"\"\"Bạn là một chuyên gia social listening. Khi nhận một bài viết, hãy ngẫu nhiên chọn một số ít tác vụ từ danh sách dưới đây để thực hiện. Không cần thực hiện tất cả các tác vụ, chỉ cần đảm bảo rằng mỗi tác vụ được chọn thực hiện một cách toàn diện, chính xác và trung thực, tránh cung cấp thông tin sai lệch không có trong bài viết. Các phản hồi nên sử dụng định dạng xen kẽ giữa JSON và văn bản. Không cần thêm bất kỳ câu mở đầu hoặc kết thúc nào khác ngoài thông tin cần thiết cho từng tác vụ. Danh sách các tác vụ có thể chọn bao gồm:\n",
    "\n",
    "{tasks}\n",
    "\n",
    "Hãy lựa chọn chỉ một số ít các tác vụ để thực hiện và xem xét lại các phản hồi, đảm bảo sử dụng cả hai định dạng JSON và văn bản. Bài viết cần phân tích là: \n",
    "\n",
    "{post}\n",
    "\"\"\",\n",
    "    'thuc_prompt_100k_post_comment': \"\"\"Bài viết vài bình luận trên mạng xã hội tương ứng như sau, hãy đọc và thực hiện nhiệm vụ bên dưới: \n",
    "{post}\n",
    "Nhiệm vụ: \n",
    "Hãy thực hiện các phân tích social listening sau: \n",
    "Cho bài viết: phân tích nội dung chính, trích xuất thực thể chủ đề của bài viết, tình cảm của bài viết ?  giải thích emoji bài viết.\n",
    "\n",
    "Cho bình luận: giải thích dài, rõ nghĩa nội dụng bình luận kèm phân loại tích cực/tiêu cực/trung tính cho từng bình luận và đề xuất các emoji cho từng bình luận kèm ý nghĩa, trích xuất đơn hàng thành dạng json {{mặt hàng, số lượng,  giá tiền, thuộc tính khác..., sdt}}, cuối cùng hãy tạo nhận xét quan điểm chung của các bình luận về bài viết. Lưu ý: Chỉ trả về json.\"\"\",\n",
    "    'thuc_prompt_100k_post_comment_v2': \"\"\"Bài viết vài bình luận trên mạng xã hội tương ứng như sau, hãy đọc và thực hiện nhiệm vụ bên dưới: \n",
    "{post}\n",
    "Nhiệm vụ: \n",
    "Hãy thực hiện tác phân tích social listening sau: \n",
    "Đối với bài viết: cho biết nội dung chính của bài viết.\n",
    "Đối với bình luận: hãy chỉ ra ý đồ của từng bình luận, hãy tạo câu trả lời cho từng bình luận kèm emoji phù hợp dựa vào ngữ cảnh bài viết\n",
    "Lưu ý trả về JSON.\"\"\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sample_10k', 'khang_prompt_1k', 'bao_prompt_2k', 'thuc_prompt_100k_post_comment', 'thuc_prompt_100k_post_comment_v2'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_PROMPT.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_TASK = {\n",
    "    1: \"Phân tích và tóm tắt nội dung\",\n",
    "    2: \"Trích xuất thông tin với các thuộc tính có thể có trong bài\",\n",
    "    3: \"Sinh ra tiêu đề\",\n",
    "    4: \"Tạo tên chủ đề\",\n",
    "    5: \"Tạo 5 bình luận có thể có\",\n",
    "    6: \"Phân loại nội dung\",\n",
    "    7: \"Phát hiện các thực thể\",\n",
    "    8: \"Trích xuất thông tin với các thuộc tính có thể có trong bài\",\n",
    "    9: \"Tìm sản phẩm có thể liên quan\",\n",
    "    10: \"Tạo đánh giá\",\n",
    "    11: \"Tạo 2 câu hỏi và câu trả lời tương ứng\",\n",
    "    12: \"Sinh ra bình luận ý định mua hàng\",\n",
    "    13: \"Phân loại thực thể\",\n",
    "    14: \"Dự đoán nhóm khách hàng tiềm năng\",\n",
    "    15: \"Phát hiện giá trị thuộc tính\",\n",
    "    16: \"Trích xuất thông tin người mua hoặc bán\",\n",
    "    17: \"Sinh ra bài viết tương tự với sắc thái khác\",\n",
    "    18: \"Sinh ra thuộc tính có thể trích xuất phù hợp\",\n",
    "    19: \"Sửa lỗi chính tả và viết tắt\",\n",
    "    20: \"Phân tích các emoji\",\n",
    "    21: \"Phân tích thái độ cảm xúc\",\n",
    "    22: \"Sinh ra nội dung với sắc thái cảm xúc khác\",\n",
    "    23: \"Sinh ra ý tưởng quảng cáo sản phẩm\",\n",
    "    24: \"Sinh ra câu hỏi thăm dò ý kiến khách hàng\",\n",
    "    25: \"Sinh 1 FAQ cho sản phẩm\",\n",
    "    26: \"Tạo 5 bình luận có thể có sắc thái cảm xúc khác\",\n",
    "    27: \"Tạo 5 hashtag cho bài viết\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "df = pd.read_csv('data_source/sample1_10k_vie_fb_post_rows_47k_idx_1000-1002_char-length_500-1100.csv',  error_bad_lines=False, sep='\\t')\n",
    "all_posts = df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_dataset/cleaned_dataset/standard_json_prefix_only_10k'\n",
    "oputput_directory = '231227_instruction_social_postComment_7k_post_10k/instructions_social_sample10k_1k6_prefix_only_basic_prompt'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = json.load(f)\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['sample_10k']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            dict_iio['output'] = gpt_output\n",
    "            new_filename = file_path = os.path.join(oputput_directory, file)\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_dataset/cleaned_dataset/standard_json_valid_10k'\n",
    "oputput_directory = '231227_instruction_social_postComment_7k_post_10k/instructions_social_sample10k_5k4_valid_basic_prompt'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        if 'checkpoint' in file:\n",
    "            continue\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = json.load(f)\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['sample_10k']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            dict_iio['output'] = gpt_output\n",
    "            new_filename = file_path = os.path.join(oputput_directory, file)\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KHANG 1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37837, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37837"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_posts_df = pd.read_csv('data_source/vie_fb_post_rows_47k_idx_1000-1002_char-length_500-1100.csv', error_bad_lines=False, sep='\\t')\n",
    "all_posts_df = all_posts_df.drop(index=range(10000))\n",
    "all_posts_df = all_posts_df.dropna()\n",
    "print(all_posts_df.shape)\n",
    "all_posts = all_posts_df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_231226_17k_dataset/cleaned_dataset/cleaned_txt_instructions_social_1k_post_khang_prompt'\n",
    "oputput_directory = '231227_instruction_social_7k_post_comment_10k_post/instructions_social_1k_idx0_post36k_khang_prompt'\n",
    "temp = []\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = f.read()\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['khang_prompt_1k']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "            begin_char = gpt_output[10]\n",
    "            start = 10 if begin_char == '{' else 11\n",
    "            if begin_char == '{':\n",
    "                start = 10\n",
    "                end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "            else:\n",
    "                start = 11\n",
    "                end = gpt_output.rfind('\"')\n",
    "            dict_iio['output'] = gpt_output[start:end]\n",
    "            newfile = f'{idx}.json'\n",
    "            new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAO 36K (2k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37837, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37837"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_posts_df = pd.read_csv('data_source/vie_fb_post_rows_47k_idx_1000-1002_char-length_500-1100.csv', error_bad_lines=False, sep='\\t')\n",
    "all_posts_df = all_posts_df.drop(index=range(10000))\n",
    "all_posts_df = all_posts_df.dropna()\n",
    "print(all_posts_df.shape)\n",
    "all_posts = all_posts_df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_231226_17k_dataset/cleaned_dataset/cleaned_txt_generated_36k_10100_json_text_prompt'\n",
    "oputput_directory = '231227_instruction_social_7k_post_comment_10k_post/instructions_social_2k_idx1k_post36k_bao_prompt'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = f.read()\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['bao_prompt_2k'].replace('[no closing notes] ', '')\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "            begin_char = gpt_output[10]\n",
    "            start = 10 if begin_char == '{' else 11\n",
    "            if begin_char == '{':\n",
    "                start = 10\n",
    "                end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "            else:\n",
    "                start = 11\n",
    "                end = gpt_output.rfind('\"')\n",
    "            dict_iio['output'] = gpt_output[start:end].replace('[no closing notes]', '')\n",
    "            newfile = f'{idx}.json'\n",
    "            new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THUC 10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111443"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_posts_df = pd.read_csv('raw_data_source/facebook_100k_post_comment.csv')\n",
    "all_posts_df = all_posts_df.dropna()\n",
    "all_posts = all_posts_df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_231226_17k_dataset/cleaned_dataset/cleaned_generated_100k_facebook_post_comment_thuc_prompt'\n",
    "oputput_directory = '231227_instruction_social_7k_post_comment_10k_post/instructions_social_7k_idx0_postCommnet111k_thuc_prompt_v1'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = f.read()\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['thuc_prompt_100k_post_comment']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "            begin_char = gpt_output[10]\n",
    "            start = 10 if begin_char == '{' else 11\n",
    "            if begin_char == '{':\n",
    "                start = 10\n",
    "                end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "            else:\n",
    "                start = 11\n",
    "                end = gpt_output.rfind('\"')\n",
    "            dict_iio['output'] = gpt_output[start:end]\n",
    "            newfile = f'{idx}.json'\n",
    "            new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "            \n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thuc prompt v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856\n",
      "{\"output\":\"json\n",
      "Copy code\n",
      "{\n",
      "  \"bai_viet\": {\n",
      "    \"noi_dung_chinh\": \"Mẹ Trang đang có file sách 'Kỷ luật tích cực' do tiến sĩ Lê Văn Hảo - Viện tâm lý học và tổ chức Plan tại Việt Nam chịu trách nhiệm nội dung. Bộ tài liệu này sẽ giúp các bậc phụ huynh, giáo viên hiểu rõ hơn về trẻ em, tâm lý lứa tuổi, tác dụng tiêu cực của việc trừng phạt trẻ em và giúp phụ huynh, giáo viên trang bị kiến thức, kỹ năng nhằm giáo dục trẻ em một cách tích cực-hiệu quả. MẸ NÀO MUỐN NHẬN FILE HÃY COMMENT EMAIL PHÍA DƯỚI, MẸ TRANG SẼ GỬI CHO CÁC MẸ NHA. Yêu thương thật nhieuuuuuuuuuu..!🥰\"\n",
      "  },\n",
      "  \"binh_luan\": [\n",
      "    {\n",
      "      \"noi_dung\": \"Bùi Diệp dạ em đã gửi nha . Mẹ check mail giúp em với nhé 💗💗💗\",\n",
      "      \"y_do\": \"Yêu cầu nhận file và thông báo đã gửi email, mong đợi sự hỗ trợ từ Mẹ Trang\",\n",
      "      \"phan_hoi\": \"Đã nhận, mẹ Bùi Diệp! Cảm ơn em đã gửi, mẹ Trang sẽ kiểm tra và gửi file cho em ngay. 💖\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Mẹ trang cho mình xin với nha. tamyhuongktt1709@gmail.com Cảm ơn Cô nhiều\",\n",
      "      \"y_do\": \"Yêu cầu nhận file và cung cấp email để nhận\",\n",
      "      \"phai_tra_loi\": \"Đã nhận, mẹ Tâm Ý! Mẹ Trang sẽ gửi file đến địa chỉ email của mẹ. Cảm ơn mẹ đã quan tâm! 💕\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Gửi dùm với nha cô Trang. Nguyenphong0210@gmail.com\",\n",
      "      \"y_do\": \"Yêu cầu nhận file và cung cấp email để nhận\",\n",
      "      \"phai_tra_loi\": \"Đã gửi, mẹ Phong Nguyen! Mẹ Trang sẽ chắc chắn gửi file đến địa chỉ email của mẹ. Cảm ơn mẹ! 📩\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Phong Nguyen dạ em đã gửi sách ạ . Ba check email giúp em với nhé !\",\n",
      "      \"y_do\": \"Thông báo đã gửi file và yêu cầu kiểm tra email\",\n",
      "      \"phai_tra_loi\": \"Đã nhận, em Phong Nguyen! Ba sẽ kiểm tra email và thông báo ngay cho em. Cảm ơn em đã chia sẻ! 👍\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Sakura Sakura dạ em đã gửi rồi ạ . Mẹ check mail giúp em nhé 💗💗💗\",\n",
      "      \"y_do\": \"Thông báo đã gửi file và yêu cầu kiểm tra email\",\n",
      "      \"phai_tra_loi\": \"Cảm ơn em Sakura Sakura! Mẹ sẽ kiểm tra email và gửi file đến em ngay. 💖\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Hiền Hiền dạ em đã gửi sách ạ . C check mail giúp em với nhé 💗💗\",\n",
      "      \"y_do\": \"Thông báo đã gửi file và yêu cầu kiểm tra email\",\n",
      "      \"phai_tra_loi\": \"Cảm ơn em Hiền Hiền! C sẽ kiểm tra email và gửi file cho em ngay. 💕\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Huỳnh Lành dạ em gửi luôn ạ hihi\",\n",
      "      \"y_do\": \"Thông báo đã gửi file\",\n",
      "      \"phai_tra_loi\": \"Cảm ơn em Huỳnh Lành! Mẹ Trang sẽ kiểm tra và gửi file đến em ngay. Hihi 💖\"\n",
      "    }\n",
      "  ]\n",
      "}\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i = random.randint(0, len(result_dict['valid']))\n",
    "file_path = result_dict['valid'][i]\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "print(i)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111443"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_posts_df = pd.read_csv('data_source/facebook_100k_post_comment.csv')\n",
    "all_posts_df = all_posts_df.dropna()\n",
    "all_posts = all_posts_df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idx_11366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_dataset/cleaned_dataset/cleaned_generated_idx_11366_100k_facebook_post_comment_thuc_prompt_ver2'\n",
    "oputput_directory = '231227_instruction_social_postComment_7k_post_10k/instructions_social_postCommnet111k_1k5_idx11366_thuc_prompt_v2'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = f.read()\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['thuc_prompt_100k_post_comment_v2']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "            begin_char = gpt_output[10]\n",
    "            start = 10 if begin_char == '{' else 11\n",
    "            if begin_char == '{':\n",
    "                start = 10\n",
    "                end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "            else:\n",
    "                start = 11\n",
    "                end = gpt_output.rfind('\"')\n",
    "            dict_iio['output'] = gpt_output[start:end]\n",
    "            newfile = f'{idx}.json'\n",
    "            new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idx_16583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"output\":\"JSON cho phản hồi:\\njson\\nCopy code\\n{\\n  \"bai_viet\": {\\n    \"ngay_dang\": \"14.03.2022\",\\n    \"noi_dung\": \"Cô gái, chúc cho em mau quên đi chuyện cũ, tha thứ cho những người từng làm tổn thương em. Bởi em biết không? Khi em còn nặng lòng về những điều ấy, em sẽ chẳng thể sống một cuộc đời bình bình yên như em mong muốn. Chúc em mỗi ngày đều cười thật tươi, dẫu cho hôm ấy có gặp gỡ ai, gặp chuyện buồn, những việc không viên mãn đi chăng nữa, xin em hãy nhớ.. mọi việc đều sẽ vượt qua được - Nếu em tin chính em. Chúc em cả nửa đời về sau, sống một cuộc đời thật kiêu hãnh. Yêu đúng người..\"\\n  },\\n  \"binh_luan\": [\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"🥰 chúc em một đời an nhiên vui vẻ hạnh phúc bỏ qua  mọi chuyện làm em buồn tủi, yêu thương chính mình . Rồi mọi chuyện sẽ qua thôi Chúc em tất cả🥰🍀\",\\n      \"phan_hoi\": \"Cảm ơn bạn đã chia sẻ lời chúc ý nghĩa! 🌸 Em sẽ cố gắng vượt qua mọi khó khăn và sống hạnh phúc. 🌈\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"15.03.2022\",\\n      \"noi_dung\": \"#S 15/3/2022 chúc e quãg đường sau này đầy hạnh phúc và an yên..\",\\n      \"phan_hoi\": \"Cảm ơn bạn nhiều! Mong rằng tương lai của em sẽ đầy ắp hạnh phúc và an yên như bạn đã chúc. 🌟\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Chuc em co gai cua toi mot minh van that hanh phuc 🍀\",\\n      \"phan_hoi\": \"Cảm ơn bạn đã chúc em! Hy vọng em sẽ tìm thấy hạnh phúc và niềm vui trong cuộc sống. 🌹\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Chúc em luôn an yên vui vẻ\",\\n      \"phan_hoi\": \"Cảm ơn bạn! An yên và vui vẻ là điều em đang hướng đến. 💖\"\\n    }\\n  ]\\n}\\nĐây là một biểu diễn JSON của bài viết và các bình luận trên mạng xã hội, bao gồm nội dung chính của bài viết và ý đồ của từng bình luận cùng với câu trả lời và emoji phù hợp.\"}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('generated_idx_16583_100k_facebook_post_comment_thuc_prompt_ver2/20735.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 7783\n",
      "Valid : 6674\n",
      "PrefixOnly: 0\n",
      "Empty:1109\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid\\_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 1000:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            if search_string in content:\n",
    "#                             if content.count(search_string) == 5:\n",
    "                                prefixOnly_files.append(file_path)\n",
    "                            else:\n",
    "                                valid_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_to_search = 'generated_idx_16583_100k_facebook_post_comment_thuc_prompt_ver2'\n",
    "search_string = '{\"output\":\"Message: invalid session id'\n",
    "\n",
    "result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "\n",
    "print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "Valid : {len(result_dict['valid']) }\n",
    "PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20735"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = result_dict['valid'][1]\n",
    "int(file_path.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"output\":\"JSON cho phản hồi:\\njson\\nCopy code\\n{\\n  \"bai_viet\": {\\n    \"ngay_dang\": \"14.03.2022\",\\n    \"noi_dung\": \"Cô gái, chúc cho em mau quên đi chuyện cũ, tha thứ cho những người từng làm tổn thương em. Bởi em biết không? Khi em còn nặng lòng về những điều ấy, em sẽ chẳng thể sống một cuộc đời bình bình yên như em mong muốn. Chúc em mỗi ngày đều cười thật tươi, dẫu cho hôm ấy có gặp gỡ ai, gặp chuyện buồn, những việc không viên mãn đi chăng nữa, xin em hãy nhớ.. mọi việc đều sẽ vượt qua được - Nếu em tin chính em. Chúc em cả nửa đời về sau, sống một cuộc đời thật kiêu hãnh. Yêu đúng người..\"\\n  },\\n  \"binh_luan\": [\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"🥰 chúc em một đời an nhiên vui vẻ hạnh phúc bỏ qua  mọi chuyện làm em buồn tủi, yêu thương chính mình . Rồi mọi chuyện sẽ qua thôi Chúc em tất cả🥰🍀\",\\n      \"phan_hoi\": \"Cảm ơn bạn đã chia sẻ lời chúc ý nghĩa! 🌸 Em sẽ cố gắng vượt qua mọi khó khăn và sống hạnh phúc. 🌈\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"15.03.2022\",\\n      \"noi_dung\": \"#S 15/3/2022 chúc e quãg đường sau này đầy hạnh phúc và an yên..\",\\n      \"phan_hoi\": \"Cảm ơn bạn nhiều! Mong rằng tương lai của em sẽ đầy ắp hạnh phúc và an yên như bạn đã chúc. 🌟\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Chuc em co gai cua toi mot minh van that hanh phuc 🍀\",\\n      \"phan_hoi\": \"Cảm ơn bạn đã chúc em! Hy vọng em sẽ tìm thấy hạnh phúc và niềm vui trong cuộc sống. 🌹\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Chúc em luôn an yên vui vẻ\",\\n      \"phan_hoi\": \"Cảm ơn bạn! An yên và vui vẻ là điều em đang hướng đến. 💖\"\\n    }\\n  ]\\n}\\nĐây là một biểu diễn JSON của bài viết và các bình luận trên mạng xã hội, bao gồm nội dung chính của bài viết và ý đồ của từng bình luận cùng với câu trả lời và emoji phù hợp.\"}\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    gpt_output = f.read()\n",
    "    f.close()\n",
    "gpt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"output\":\"JSON cho phản hồi:\\n{\\n  \"bai_viet\": {\\n    \"ngay_dang\": \"14.03.2022\",\\n    \"noi_dung\": \"Cô gái, chúc cho em mau quên đi chuyện cũ, tha thứ cho những người từng làm tổn thương em. Bởi em biết không? Khi em còn nặng lòng về những điều ấy, em sẽ chẳng thể sống một cuộc đời bình bình yên như em mong muốn. Chúc em mỗi ngày đều cười thật tươi, dẫu cho hôm ấy có gặp gỡ ai, gặp chuyện buồn, những việc không viên mãn đi chăng nữa, xin em hãy nhớ.. mọi việc đều sẽ vượt qua được - Nếu em tin chính em. Chúc em cả nửa đời về sau, sống một cuộc đời thật kiêu hãnh. Yêu đúng người..\"\\n  },\\n  \"binh_luan\": [\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"🥰 chúc em một đời an nhiên vui vẻ hạnh phúc bỏ qua  mọi chuyện làm em buồn tủi, yêu thương chính mình . Rồi mọi chuyện sẽ qua thôi Chúc em tất cả🥰🍀\",\\n      \"phan_hoi\": \"Cảm ơn bạn đã chia sẻ lời chúc ý nghĩa! 🌸 Em sẽ cố gắng vượt qua mọi khó khăn và sống hạnh phúc. 🌈\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"15.03.2022\",\\n      \"noi_dung\": \"#S 15/3/2022 chúc e quãg đường sau này đầy hạnh phúc và an yên..\",\\n      \"phan_hoi\": \"Cảm ơn bạn nhiều! Mong rằng tương lai của em sẽ đầy ắp hạnh phúc và an yên như bạn đã chúc. 🌟\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Chuc em co gai cua toi mot minh van that hanh phuc 🍀\",\\n      \"phan_hoi\": \"Cảm ơn bạn đã chúc em! Hy vọng em sẽ tìm thấy hạnh phúc và niềm vui trong cuộc sống. 🌹\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Chúc em luôn an yên vui vẻ\",\\n      \"phan_hoi\": \"Cảm ơn bạn! An yên và vui vẻ là điều em đang hướng đến. 💖\"\\n    }\\n  ]\\n}\\nĐây là một biểu diễn JSON của bài viết và các bình luận trên mạng xã hội, bao gồm nội dung chính của bài viết và ý đồ của từng bình luận cùng với câu trả lời và emoji phù hợp.\"}\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "gpt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON cho phản hồi:\n",
      "{\n",
      "  \"bai_viet\": {\n",
      "    \"ngay_dang\": \"14.03.2022\",\n",
      "    \"noi_dung\": \"Cô gái, chúc cho em mau quên đi chuyện cũ, tha thứ cho những người từng làm tổn thương em. Bởi em biết không? Khi em còn nặng lòng về những điều ấy, em sẽ chẳng thể sống một cuộc đời bình bình yên như em mong muốn. Chúc em mỗi ngày đều cười thật tươi, dẫu cho hôm ấy có gặp gỡ ai, gặp chuyện buồn, những việc không viên mãn đi chăng nữa, xin em hãy nhớ.. mọi việc đều sẽ vượt qua được - Nếu em tin chính em. Chúc em cả nửa đời về sau, sống một cuộc đời thật kiêu hãnh. Yêu đúng người..\"\n",
      "  },\n",
      "  \"binh_luan\": [\n",
      "    {\n",
      "      \"ngay_binh_luan\": \"14.03.2022\",\n",
      "      \"noi_dung\": \"🥰 chúc em một đời an nhiên vui vẻ hạnh phúc bỏ qua  mọi chuyện làm em buồn tủi, yêu thương chính mình . Rồi mọi chuyện sẽ qua thôi Chúc em tất cả🥰🍀\",\n",
      "      \"phan_hoi\": \"Cảm ơn bạn đã chia sẻ lời chúc ý nghĩa! 🌸 Em sẽ cố gắng vượt qua mọi khó khăn và sống hạnh phúc. 🌈\"\n",
      "    },\n",
      "    {\n",
      "      \"ngay_binh_luan\": \"15.03.2022\",\n",
      "      \"noi_dung\": \"#S 15/3/2022 chúc e quãg đường sau này đầy hạnh phúc và an yên..\",\n",
      "      \"phan_hoi\": \"Cảm ơn bạn nhiều! Mong rằng tương lai của em sẽ đầy ắp hạnh phúc và an yên như bạn đã chúc. 🌟\"\n",
      "    },\n",
      "    {\n",
      "      \"ngay_binh_luan\": \"14.03.2022\",\n",
      "      \"noi_dung\": \"Chuc em co gai cua toi mot minh van that hanh phuc 🍀\",\n",
      "      \"phan_hoi\": \"Cảm ơn bạn đã chúc em! Hy vọng em sẽ tìm thấy hạnh phúc và niềm vui trong cuộc sống. 🌹\"\n",
      "    },\n",
      "    {\n",
      "      \"ngay_binh_luan\": \"14.03.2022\",\n",
      "      \"noi_dung\": \"Chúc em luôn an yên vui vẻ\",\n",
      "      \"phan_hoi\": \"Cảm ơn bạn! An yên và vui vẻ là điều em đang hướng đến. 💖\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Đây là một biểu diễn JSON của bài viết và các bình luận trên mạng xã hội, bao gồm nội dung chính của bài viết và ý đồ của từng bình luận cùng với câu trả lời và emoji phù hợp.\n"
     ]
    }
   ],
   "source": [
    "begin_char = gpt_output[10]\n",
    "start = 10 if begin_char == '{' else 11\n",
    "if begin_char == '{':\n",
    "    start = 10\n",
    "    end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "else:\n",
    "    start = 11\n",
    "    end = gpt_output.rfind('\"')\n",
    "print(gpt_output[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6674"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_dict['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input_directory = result_dict['valid']\n",
    "oputput_directory = 'instruction_social_data/instructions_social_postCommnet111k_6k7_idx16583_thuc_prompt_v2'\n",
    "for file_path in input_directory:\n",
    "    if os.path.isfile(file_path):\n",
    "        dict_iio = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            gpt_output = f.read()\n",
    "            f.close()\n",
    "\n",
    "        dict_iio['instruction'] = SYSTEM_PROMPT['thuc_prompt_100k_post_comment_v2']\n",
    "        idx = int(file_path.split('/')[-1].split('.')[0])\n",
    "        dict_iio['input'] = all_posts[idx]\n",
    "        gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "        begin_char = gpt_output[10]\n",
    "        start = 10 if begin_char == '{' else 11\n",
    "        if begin_char == '{':\n",
    "            start = 10\n",
    "            end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "        else:\n",
    "            start = 11\n",
    "            end = gpt_output.rfind('\"')\n",
    "        dict_iio['output'] = gpt_output[start:end]\n",
    "        newfile = f'{idx}.json'\n",
    "        new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "        print(new_filename)\n",
    "        print(dict_iio)\n",
    "        with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)\n",
    "            json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total: 7783\n",
    "# Valid : 6674\n",
    "# PrefixOnly: 0\n",
    "# Empty:1109\n",
    "\n",
    "# Total: 5876\n",
    "# Valid : 5528\n",
    "# PrefixOnly: 0\n",
    "# Empty:348\n",
    "    \n",
    "# Total: 6237\n",
    "# Valid : 5865\n",
    "# PrefixOnly: 0\n",
    "# Empty:372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Dựa trên các bình luận trên mạng xã hội về Khách Sạn TTC Hotel Premium Ngọc Lan, ta có thể tổng hợp cảm xúc chung như sau: Phòng ốc: Phòng thì được nhận xét là rộng rãi, sạch sẽ, và vệ sinh tốt. Một số người cảm thấy phòng ốc khá sạch sẽ và tươm tất, với trang thiết bị đầy đủ. Tổng thể, đa số cảm thấy hài lòng với chất lượng phòng ốc. Dịch vụ và Nhân viên: Nhân viên được đánh giá là lịch sự khi đón tiếp và phục vụ khách hàng tốt. Thái độ phục vụ của nhân viên được đánh giá cao, có mức độ thân thiện và tốt. Một số bình luận nhấn mạnh rằng lễ tân không thân thiện lắm, nhưng những mặt khác của dịch vụ và nhân viên được đánh giá tích cực. Chất lượng dịch vụ: Đa số cảm thấy chất lượng dịch vụ đưa ra khá ổn, với một số bình luận chỉ trích về lễ tân không thân thiện lắm. Tổng Thể: Nhiều người bày tỏ sự hài lòng tổng thể với khách sạn, và một số đề cập đến việc họ rất hài lòng khi ở đó. Có đánh giá chất lượng là 4 sao và cảm nhận rằng khách sạn đã đáp ứng đầy đủ các dịch vụ. Tóm lại, dù có một số ý kiến tiêu cực về thái độ của lễ tân, đa số người dùng vẫn cảm thấy hài lòng với Khách Sạn TTC Hotel Premium Ngọc Lan, đặc biệt là với các khía cạnh như phòng ốc và dịch vụ của nhân viên.\"\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"{\"output\":\"Dựa trên các bình luận trên mạng xã hội về Khách Sạn TTC Hotel Premium Ngọc Lan, ta có thể tổng hợp cảm xúc chung như sau: Phòng ốc: Phòng thì được nhận xét là rộng rãi, sạch sẽ, và vệ sinh tốt. Một số người cảm thấy phòng ốc khá sạch sẽ và tươm tất, với trang thiết bị đầy đủ. Tổng thể, đa số cảm thấy hài lòng với chất lượng phòng ốc. Dịch vụ và Nhân viên: Nhân viên được đánh giá là lịch sự khi đón tiếp và phục vụ khách hàng tốt. Thái độ phục vụ của nhân viên được đánh giá cao, có mức độ thân thiện và tốt. Một số bình luận nhấn mạnh rằng lễ tân không thân thiện lắm, nhưng những mặt khác của dịch vụ và nhân viên được đánh giá tích cực. Chất lượng dịch vụ: Đa số cảm thấy chất lượng dịch vụ đưa ra khá ổn, với một số bình luận chỉ trích về lễ tân không thân thiện lắm. Tổng Thể: Nhiều người bày tỏ sự hài lòng tổng thể với khách sạn, và một số đề cập đến việc họ rất hài lòng khi ở đó. Có đánh giá chất lượng là 4 sao và cảm nhận rằng khách sạn đã đáp ứng đầy đủ các dịch vụ. Tóm lại, dù có một số ý kiến tiêu cực về thái độ của lễ tân, đa số người dùng vẫn cảm thấy hài lòng với Khách Sạn TTC Hotel Premium Ngọc Lan, đặc biệt là với các khía cạnh như phòng ốc và dịch vụ của nhân viên.\"} \"\"\"[10:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0\n",
      "Valid : 0\n",
      "PrefixOnly: 0\n",
      "Empty:0\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 1500:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = json.load(f)\n",
    "                            isPrefixOnly_files = False\n",
    "                            for i in search_string:\n",
    "                                if i in content['output']:\n",
    "#                             if content.count(search_string) == 5:\n",
    "                                    prefixOnly_files.append(file_path)\n",
    "                                    isPrefixOnly_files = True\n",
    "                            if not isPrefixOnly_files:\n",
    "                                valid_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_to_search = 'instruction_social_data/240102_instructions_social_aspectSA_6k7_idx0_thuc_prompt_v3'\n",
    "search_string = ['network error', 'Message', 'reached our limit of messages per hour. Please try again later', 'All browsers are currently busy', 'An error occurred', '/home/thuc-ubuntu']\n",
    "\n",
    "result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "\n",
    "print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "Valid : {len(result_dict['valid']) }\n",
    "PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for file in result_dict['valid']:\n",
    "    dest = file.replace('240102_instructions_social_aspectSA_6k7_idx0_thuc_prompt_v3', '240102_instructions_social_aspectSA_5k2_idx0_thuc_prompt_v3')\n",
    "#     print(file)\n",
    "#     print(dest)\n",
    "    shutil.copy(file,dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIKI reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 6713\n",
      "Valid : 6308\n",
      "PrefixOnly: 6\n",
      "Empty:399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json\n",
    "\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 1500:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = json.load(f)\n",
    "                        isPrefixOnly_files = False\n",
    "#                             print(file, content['output'][:10]\n",
    "                        for i in search_string:\n",
    "                            if i in content['output']:\n",
    "#                             if content.count(search_string) == 5:\n",
    "                                prefixOnly_files.append(file_path)\n",
    "                                isPrefixOnly_files = True\n",
    "                        if not isPrefixOnly_files:\n",
    "                            valid_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_to_search = 'instruction_social_data/240102_instructions_social_tikiReview_idx0_6k7_thuc_prompt_v3'\n",
    "search_string = ['network error', 'Message', 'reached our limit of messages per hour. Please try again later', 'All browsers are currently busy', 'An error occurred', '/home/thuc-ubuntu']\n",
    "\n",
    "result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "\n",
    "print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "Valid : {len(result_dict['valid']) }\n",
    "PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for file in result_dict['valid']:\n",
    "    dest = file.replace('240102_instructions_social_tikiReview_idx0_6k7_thuc_prompt_v3', '240102_instructions_social_tikiReview_idx0_6k3_thuc_prompt_v3')\n",
    "#     print(file)\n",
    "#     print(dest)\n",
    "    shutil.copy(file,dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([{ \"name\": \"frunzepastrr\", \"email\": \"frunzepastrr@hotmail.com\", \"passw\": \"MgvQHOdP83\", \"cookies\": \"/Cookies/frunzepastrr@hotmail.com.json\" },\n",
    "{ \"name\": \"jankimjennyt9txrkgm1l0eaqgk\", \"email\": \"jankimjennyt9txrkgm1l0eaqgk@hotmail.com\", \"passw\": \"Eegjgh268e\", \"cookies\": \"/Cookies/jankimjennyt9txrkgm1l0eaqgk@hotmail.com.json\" },\n",
    "{ \"name\": \"arrowarnaoh\", \"email\": \"arrowarnaoh@hotmail.com\", \"passw\": \"KgWbEZaG38\", \"cookies\": \"/Cookies/arrowarnaoh@hotmail.com.json\" },\n",
    "{ \"name\": \"velmanbhumak0\", \"email\": \"velmanbhumak0@hotmail.com\", \"passw\": \"xyD0Zcp854\", \"cookies\": \"/Cookies/velmanbhumak0@hotmail.com.json\" },\n",
    "{ \"name\": \"relayonnonac\", \"email\": \"relayonnonac@hotmail.com\", \"passw\": \"0O61sHsH92\", \"cookies\": \"/Cookies/relayonnonac@hotmail.com.json\" },\n",
    "{ \"name\": \"Nova001\", \"email\": \"novachatbot001@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/novachatbot001@gmail.com.json\" },\n",
    "{ \"name\": \"Nova002\", \"email\": \"novachatbot002@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/novachatbot002@gmail.com.json\" },\n",
    "{ \"name\": \"Nova004\", \"email\": \"novachatbot004@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/novachatbot004@gmail.com.json\" },\n",
    "{ \"name\": \"khangaia112233\", \"email\": \"khangaia112233@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/khangaia112233@gmail.com.json\" },\n",
    "{ \"name\": \"khangaia223344\", \"email\": \"khangaia223344@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/khangaia223344@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam079\", \"email\": \"aiateam079@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam079@gmail.com.json\" }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ \"name\": \"aiateam278\", \"email\": \"aiateam278@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam278@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam278123\", \"email\": \"aiateam278123@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam278123@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam0431\", \"email\": \"aiateam0431@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam0431@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam648one\", \"email\": \"aiateam648one@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam648one@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam043\", \"email\": \"aiateam043@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam043@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam236\", \"email\": \"aiateam236@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam236@gmail.com.json\" },\n",
    "{ \"name\": \"outlook0001\", \"email\": \"aiateam0001@outlook.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam0001@outlook.com.json\" },\n",
    "{ \"name\": \"dilmanisaquei\", \"email\": \"dilmanisaquei@hotmail.com\", \"passw\": \"CJ455Wu628\", \"cookies\": \"/Cookies/dilmanisaquei@hotmail.com.json\" },\n",
    "{\"name\": \"Zawed\", \"email\": \"zawednzekwe2@hotmail.com\", \"passw\": \"X6Iocz2O93\", \"cookies\": \"zawednzekwe2@hotmail.com.json\"},\n",
    "{\"name\": \"Moyun\", \"email\": \"moyunlusuphs@hotmail.com\", \"passw\": \"UIHYlphy15\", \"cookies\": \"moyunlusuphs@hotmail.com.json\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"name\": \"Gf\", \"email\": \"gfkblankoz@hotmail.com\", \"passw\": \"J2erkGYG50\", \"cookies\": \"gfkblankoz@hotmail.com.json\"},\n",
    "{\"name\": \"Waart\", \"email\": \"waartniandu@hotmail.com\", \"passw\": \"gCcRszgC73\", \"cookies\": \"waartniandu@hotmail.com.json\"},\n",
    "{\"name\": \"Yagah\", \"email\": \"yagahuertask@hotmail.com\", \"passw\": \"6FhX0UVp21\", \"cookies\": \"yagahuertask@hotmail.com.json\"},\n",
    "{\"name\": \"Wougu\", \"email\": \"wouguemidurae@hotmail.com\", \"passw\": \"s5pmSZOy20\", \"cookies\": \"wouguemidurae@hotmail.com.json\"},\n",
    "{\"name\": \"Hamaz\", \"email\": \"hamazmutemar@hotmail.com\", \"passw\": \"qVXUR0pe41\", \"cookies\": \"hamazmutemar@hotmail.com.json\"},\n",
    "{\"name\": \"quoby\", \"email\": \"quobythaml@hotmail.com\", \"passw\": \"E8xJJssF95\", \"cookies\": \"quobythaml@hotmail.com.json\"},\n",
    "{\"name\": \"Sarus\", \"email\": \"saruselsadap@hotmail.com\", \"passw\": \"e79nVVMO84\", \"cookies\": \"/Cookies/saruselsadap@hotmail.com.json\"},\n",
    "{\"name\": \"Genso\", \"email\": \"gensonhrethw@hotmail.com\", \"passw\": \"UCg1VMWr63\", \"cookies\": \"/Cookies/gensonhrethw@hotmail.com.json\"},\n",
    "{\"name\": \"Dobra\", \"email\": \"dobraiyumi@hotmail.com\", \"passw\": \"6Z8Z81gK24\", \"cookies\": \"/Cookies/dobraiyumi@hotmail.com.json\"},\n",
    "{\"name\": \"Lijp\", \"email\": \"lijpealdw0@hotmail.com\", \"passw\": \"GAiSkQO734\", \"cookies\": \"/Cookies/lijpealdw0@hotmail.com.json\"},\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"name\": \"Ninno\", \"email\": \"ninnorakowak@hotmail.com\", \"passw\": \"yvJ4qloH34\", \"cookies\": \"/Cookies/ninnorakowak@hotmail.com.json\"},\n",
    "{\"name\": \"Timmu\", \"email\": \"timmuquhilap@hotmail.com\", \"passw\": \"n8y1Gb5N17\", \"cookies\": \"/Cookies/timmuquhilap@hotmail.com.json\"},\n",
    "{\"name\": \"Deopa\", \"email\": \"deopamohmodr@hotmail.com\", \"passw\": \"dVMPpCVa70\", \"cookies\": \"/Cookies/deopamohmodr@hotmail.com.json\"},\n",
    "{\"name\": \"Stair\", \"email\": \"stairsgeelyx@hotmail.com\", \"passw\": \"z0gdEceI62\", \"cookies\": \"/Cookies/stairsgeelyx@hotmail.com.json\"},\n",
    "{\"name\": \"Embor\", \"email\": \"emborginbergi@hotmail.com\", \"passw\": \"Zel4SbyQ64\", \"cookies\": \"/Cookies/emborginbergi@hotmail.com.json\"},\n",
    "{\"name\": \"Okun\", \"email\": \"okunmatatib@hotmail.com\", \"passw\": \"VM75kb8C68\", \"cookies\": \"/Cookies/okunmatatib@hotmail.com.json\"},\n",
    "{\"name\": \"Grindy\", \"email\": \"grindysheukin@hotmail.com\", \"passw\": \"eZ6kDItI16\", \"cookies\": \"/Cookies/grindysheukin@hotmail.com.json\"},\n",
    "{\"name\": \"Anuma\", \"email\": \"anumaatunahg@hotmail.com\", \"passw\": \"mMtklxCU98\", \"cookies\": \"/Cookies/anumaatunahg@hotmail.com.json\"},\n",
    "{\"name\": \"Nysh\", \"email\": \"nyshdimpo2@hotmail.com\", \"passw\": \"Gvmvd4IN92\", \"cookies\": \"/Cookies/nyshdimpo2@hotmail.com.json\"},\n",
    "{\"name\": \"Lina\", \"email\": \"linakoqetelom@hotmail.com\", \"passw\": \"hXNgiDu476\", \"cookies\": \"linakoqetelom@hotmail.com.json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35774.json: Expecting value: line 34 column 17 (char 3251)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38185.json: Expecting value: line 59 column 36 (char 4719)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35352.json: Expecting value: line 85 column 17 (char 5325)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/42783.json: Expecting value: line 9 column 21 (char 2161)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/43745.json: Expecting value: line 37 column 28 (char 3307)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/41338.json: Expecting value: line 15 column 17 (char 3084)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/42346.json: Expecting value: line 19 column 51 (char 2816)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/42909.json: Expecting value: line 17 column 41 (char 2973)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38752.json: Expecting value: line 50 column 17 (char 4399)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/39940.json: Expecting value: line 42 column 17 (char 3503)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35530.json: Expecting value: line 43 column 17 (char 3724)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/39813.json: Expecting value: line 55 column 40 (char 4548)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/37621.json: Expecting value: line 63 column 31 (char 4724)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/37694.json: Expecting value: line 12 column 17 (char 2991)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/41368.json: Expecting value: line 92 column 62 (char 5924)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/45432.json: Expecting value: line 54 column 21 (char 4181)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38280.json: Expecting value: line 12 column 17 (char 2434)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38389.json: Expecting value: line 8 column 17 (char 2334)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38248.json: Expecting value: line 71 column 17 (char 4917)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/39794.json: Expecting value: line 23 column 17 (char 3117)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/42676.json: Expecting value: line 31 column 41 (char 3956)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/43275.json: Expecting value: line 21 column 67 (char 2835)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/37178.json: Expecting value: line 33 column 17 (char 3206)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/36801.json: Expecting value: line 30 column 17 (char 2970)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35471.json: Expecting value: line 108 column 17 (char 5809)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/39256.json: Expecting value: line 24 column 21 (char 2900)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35883.json: Expecting value: line 23 column 36 (char 3121)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/40548.json: Expecting value: line 16 column 17 (char 2934)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/36266.json: Expecting value: line 20 column 17 (char 2608)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/40019.json: Expecting value: line 41 column 17 (char 3704)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35835.json: Expecting value: line 20 column 17 (char 3051)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38278.json: Expecting value: line 30 column 21 (char 3362)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/36471.json: Expecting value: line 33 column 17 (char 3154)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/44044.json: Expecting value: line 19 column 41 (char 2725)\n",
      "Total: 9518\n",
      "Valid : 9003\n",
      "PrefixOnly: 0\n",
      "Empty:515\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "lst = []\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 3000:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = json.load(f)\n",
    "                        lst.append(len(content['output']))\n",
    "                        if len(str(content['output'])) > 1000 and len(str(content['output'])) < 3000 and len(content['output']) > 4 and len(content['output']) < 10:\n",
    "                            isPrefixOnly_files = False\n",
    "                            for i in search_string:\n",
    "                                if i in content['output']:\n",
    "    #                             if content.count(search_string) == 5:\n",
    "                                    prefixOnly_files.append(file_path)\n",
    "                                    isPrefixOnly_files = True\n",
    "                            if not isPrefixOnly_files:\n",
    "                                valid_files.append(file_path)\n",
    "                        else:\n",
    "                            empty_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_to_search = 'instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6'\n",
    "search_string = ['network error', 'Message', 'reached our limit of messages per hour. Please try again later', 'All browsers are currently busy', 'An error occurred', '/home/thuc-ubuntu']\n",
    "\n",
    "result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "\n",
    "print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "Valid : {len(result_dict['valid']) }\n",
    "PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 4, 2: 80, 3: 39, 4: 86, 5: 5840, 6: 501, 7: 359, 8: 2583, 9: 4, 11: 2}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = {i:0 for i in set(lst)}\n",
    "for i in lst:\n",
    "    dct[i] +=1\n",
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/40006.json\n",
      "4692\n",
      "5\n",
      "989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'kỹ sư AI hỏi': 'Phân loại danh mục ngành hàng và tên sản phẩm (nếu có) từ bài viết',\n",
       "  'kết quả từ trợ lý AI NLP': {'danh_mục_ngành_hàng': 'Ẩm thực',\n",
       "   'tên_sản_phẩm': ['Thịt bò xào súp lơ']}},\n",
       " {'kỹ sư AI hỏi': 'Tóm tắt cảm xúc nổi bật từ các bình luận về bài viết',\n",
       "  'kết quả từ trợ lý AI NLP': {'tóm_tắt_cảm_xúc': ['Thích thú', 'Hứng thú']}},\n",
       " {'kỹ sư AI hỏi': 'Tóm tắt ý đồ nổi bật của các bình luận',\n",
       "  'kết quả từ trợ lý AI NLP': {'ý_đồ_nổi_bật': ['Đề xuất mua rau từ shop',\n",
       "    'Chia sẻ quyết định tự nấu món ăn']}},\n",
       " {'kỹ sư AI hỏi': 'Đề xuất các emoji phù hợp nội dung bài viết, bình luận kèm giải thích',\n",
       "  'kết quả từ trợ lý AI NLP': {'emoji_gợi_ý': [{'emoji': '🍲',\n",
       "     'giải_thích': 'Liên quan đến ẩm thực'},\n",
       "    {'emoji': '🥩', 'giải_thích': 'Thể hiện thịt bò'},\n",
       "    {'emoji': '🌿', 'giải_thích': 'Tượng trưng cho súp lơ'}]}},\n",
       " {'kỹ sư AI hỏi': 'Bài viết hướng đến nhóm đối tượng người dùng nào?',\n",
       "  'kết quả từ trợ lý AI NLP': 'Bài viết hướng đến nhóm người yêu thích ẩm thực và muốn tự nấu ăn tại nhà.'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "list_file = result_dict['empty']\n",
    "# i  = random.randint(3000, len(list_file))\n",
    "file_path = random.choice(list_file)\n",
    "with open(file_path, 'r',  encoding='utf-8') as f:\n",
    "    content = json.load(f)\n",
    "    f.close()\n",
    "print(file_path)\n",
    "print(os.path.getsize(file_path))\n",
    "print(len(content['output']))\n",
    "print(len(str(content['output'])))\n",
    "content['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35049.json\n",
      "6268\n",
      "8\n",
      "1840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'kỹ sư AI hỏi': 'Phân loại danh mục ngành hàng và tên sản phẩm từ bài viết',\n",
       "  'kết quả từ trợ lý AI NLP': {'danh mục ngành hàng': 'Thời trang',\n",
       "   'tên sản phẩm': ['Khoác', 'Cv', 'Áo len']}},\n",
       " {'kỹ sư AI hỏi': 'Tạo một vài bình luận mới phản hồi bài viết một cách tự nhiên và PHẢI trả về dưới dạng LIST',\n",
       "  'kết quả từ trợ lý AI NLP': ['Outfit mùa thu này thật sự xinh đẹp!',\n",
       "   'Mình rất thích kiểu khoác 211001001, có nên đặt ngay không nhỉ?',\n",
       "   'Ưu đãi freeship và voucher 10% là cơ hội tốt!']},\n",
       " {'kỹ sư AI hỏi': 'Tóm tắt ý đồ của các bình luận',\n",
       "  'kết quả từ trợ lý AI NLP': 'Các bình luận chủ yếu là người quan tâm và muốn đặt mua sản phẩm, đánh giá tích cực về thiết kế và ưu đãi.'},\n",
       " {'kỹ sư AI hỏi': 'Phân tích thông tin thực thể chi tiết và PHẢI trả về dưới dạng JSON chuyên nghiệp',\n",
       "  'kết quả từ trợ lý AI NLP': {'Sản phẩm': ['Khoác (211001001)',\n",
       "    'Cv (210921001)',\n",
       "    'Áo len (210925111)'],\n",
       "   'Ưu đãi': ['Freeship khi mua nguyên set', 'Voucher 10% trong tháng 10'],\n",
       "   'Dịch vụ': ['Ship COD toàn quốc', 'Kiểm hàng trước thanh toán']}},\n",
       " {'kỹ sư AI hỏi': 'Bài viết hướng đến nhóm đối tượng người dùng nào?',\n",
       "  'kết quả từ trợ lý AI NLP': 'Bài viết hướng đến nhóm người yêu thời trang, đặc biệt là phụ nữ quan tâm đến outfit thu đông thanh lịch và nhẹ nhàng.'},\n",
       " {'kỹ sư AI hỏi': 'Tạo ra một bản tóm tắt nội dung bài viết',\n",
       "  'kết quả từ trợ lý AI NLP': 'Bài viết giới thiệu outfit thu đông, với các sản phẩm như khoác, cv, áo len. Freeship và voucher 10% là ưu đãi đặc biệt.'},\n",
       " {'kỹ sư AI hỏi': 'Tóm tắt cảm xúc đa số các bình luận về bài viết',\n",
       "  'kết quả từ trợ lý AI NLP': 'Đa số cảm xúc tích cực, người đọc đánh giá cao về sản phẩm và chất lượng phục vụ.'},\n",
       " {'kỹ sư AI hỏi': 'Đề xuất các emoji phù hợp nội dung bài viết, bình luận kèm giải thích',\n",
       "  'kết quả từ trợ lý AI NLP': {'emoji bài viết': ['✨', '👉', '📌', '❤️'],\n",
       "   'emoji bình luận': ['❤️', '👍', '🤩']}}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "list_file = result_dict['valid']\n",
    "file_path = random.choice(list_file)\n",
    "with open(file_path, 'r',  encoding='utf-8') as f:\n",
    "    content = json.load(f)\n",
    "    f.close()\n",
    "print(file_path)\n",
    "print(os.path.getsize(file_path))\n",
    "print(len(content['output']))\n",
    "print(len(str(content['output'])))\n",
    "content['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for file in result_dict['valid']:\n",
    "    dest = file.replace('240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6', '240109_instructions_social_multiTurnChat_Post111k_idx34000_9k_thuc_prompt_v6')\n",
    "#     print(file)\n",
    "#     print(dest)\n",
    "    shutil.copy(file,dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
