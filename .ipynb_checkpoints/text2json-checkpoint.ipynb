{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster to valid, prefixOnly, empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 1000:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "#                             if search_string in content:\n",
    "                            if content.count(search_string) == 5:\n",
    "                                prefixOnly_files.append(file_path)\n",
    "                            else:\n",
    "                                valid_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "\n",
    "# def txt2json(list_of_file_names):\n",
    "#     incorrect_list = []\n",
    "#     for filename in list_of_file_names:\n",
    "#         try:\n",
    "#             with open(f'{filename}', 'r', encoding='utf-8') as file:\n",
    "#                 content = file.read()\n",
    "#             content = content.replace(\"json\\nCopy code\\n\", \"\").replace('''\"{''', \"{\").replace('''}\"''', \"}\")\n",
    "#             json_object = json.loads(content, strict=False)\n",
    "#             new_filename = filename.replace('.txt', '.json').replace(directory, 'test')\n",
    "#             with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "#                 json.dump(json_object, json_file, ensure_ascii=False, indent=4)\n",
    "#         except:\n",
    "#             incorrect_list.append(filename)\n",
    "#     return incorrect_list\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# directory_to_search = 'instructions_social_1k_post_khang_prompt'\n",
    "# search_string = 'D·ª±a v√†o √Ω c·ªßa task'\n",
    "\n",
    "# result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "# # fail_files = txt2json(result_dict['valid'])\n",
    "\n",
    "# print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "# Valid : {len(result_dict['valid']) - len(fail_files)}\n",
    "# Fail: {len(fail_files)}\n",
    "# PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "# Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can NOT found json format.\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import re\n",
    "\n",
    "# def extract_json_from_text(text):\n",
    "#     match = re.search(r'\\{.*\\}', text)\n",
    "\n",
    "#     if match:\n",
    "#         json_string = match.group()\n",
    "#         try:\n",
    "#             json_data = json.loads(json_string)\n",
    "#             return json_data\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"Error decoding JSON: {e}\")\n",
    "#             return None\n",
    "#     else:\n",
    "#         print(\"Can NOT found json format.\")\n",
    "#         return None\n",
    "\n",
    "    \n",
    "# path = fail_files[0]\n",
    "# with open(path, 'r', encoding='utf-8') as f:\n",
    "#     content = f.read()\n",
    "# result = extract_json_from_text(content)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if sub_str in string\n",
    "# Total: 7065\n",
    "# Valid : 4044\n",
    "# Fail: 248\n",
    "# PrefixOnly: 1447\n",
    "# Empty:1326\n",
    "    \n",
    "    \n",
    "# if string contain 5 sub_str(s)\n",
    "# Total: 7071\n",
    "# Valid : 4105\n",
    "# Fail: 287\n",
    "# PrefixOnly: 1352\n",
    "# Empty:1327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_nth(haystack: str, needle: str, n: int, reverse: bool = False) -> int:\n",
    "#     if not reverse:\n",
    "#         start = haystack.find(needle)\n",
    "#         while start >= 0 and n > 1:\n",
    "#             start = haystack.find(needle, start + len(needle))\n",
    "#             n -= 1\n",
    "#     else:\n",
    "#         start = haystack.rfind(needle)\n",
    "#         while start >= 0 and n > 1:\n",
    "#             start = haystack.rfind(needle, 0, start)\n",
    "#             n -= 1\n",
    "#     return start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefix Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {\n",
    "    1: \"Ph√¢n t√≠ch v√† t√≥m t·∫Øt n·ªôi dung\",\n",
    "    2: \"Tr√≠ch xu·∫•t th√¥ng tin v·ªõi c√°c thu·ªôc t√≠nh c√≥ th·ªÉ c√≥ trong b√†i\",\n",
    "    3: \"Sinh ra ti√™u ƒë·ªÅ\",\n",
    "    4: \"T·∫°o t√™n ch·ªß ƒë·ªÅ\",\n",
    "    5: \"T·∫°o 5 b√¨nh lu·∫≠n c√≥ th·ªÉ c√≥\",\n",
    "    6: \"Ph√¢n lo·∫°i n·ªôi dung\",\n",
    "    7: \"Ph√°t hi·ªán c√°c th·ª±c th·ªÉ\",\n",
    "    8: \"Tr√≠ch xu·∫•t th√¥ng tin v·ªõi c√°c thu·ªôc t√≠nh c√≥ th·ªÉ c√≥ trong b√†i\",\n",
    "    9: \"T√¨m s·∫£n ph·∫©m c√≥ th·ªÉ li√™n quan\",\n",
    "    10: \"T·∫°o ƒë√°nh gi√°\",\n",
    "    11: \"T·∫°o 2 c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi t∆∞∆°ng ·ª©ng\",\n",
    "    12: \"Sinh ra b√¨nh lu·∫≠n √Ω ƒë·ªãnh mua h√†ng\",\n",
    "    13: \"Ph√¢n lo·∫°i th·ª±c th·ªÉ\",\n",
    "    14: \"D·ª± ƒëo√°n nh√≥m kh√°ch h√†ng ti·ªÅm nƒÉng\",\n",
    "    15: \"Ph√°t hi·ªán gi√° tr·ªã thu·ªôc t√≠nh\",\n",
    "    16: \"Tr√≠ch xu·∫•t th√¥ng tin ng∆∞·ªùi mua ho·∫∑c b√°n\",\n",
    "    17: \"Sinh ra b√†i vi·∫øt t∆∞∆°ng t·ª± v·ªõi s·∫Øc th√°i kh√°c\",\n",
    "    18: \"Sinh ra thu·ªôc t√≠nh c√≥ th·ªÉ tr√≠ch xu·∫•t ph√π h·ª£p\",\n",
    "    19: \"S·ª≠a l·ªói ch√≠nh t·∫£ v√† vi·∫øt t·∫Øt\",\n",
    "    20: \"Ph√¢n t√≠ch c√°c emoji\",\n",
    "    21: \"Ph√¢n t√≠ch th√°i ƒë·ªô c·∫£m x√∫c\",\n",
    "    22: \"Sinh ra n·ªôi dung v·ªõi s·∫Øc th√°i c·∫£m x√∫c kh√°c\",\n",
    "    23: \"Sinh ra √Ω t∆∞·ªüng qu·∫£ng c√°o s·∫£n ph·∫©m\",\n",
    "    24: \"Sinh ra c√¢u h·ªèi thƒÉm d√≤ √Ω ki·∫øn kh√°ch h√†ng\",\n",
    "    25: \"Sinh 1 FAQ cho s·∫£n ph·∫©m\",\n",
    "    26: \"T·∫°o 5 b√¨nh lu·∫≠n c√≥ th·ªÉ c√≥ s·∫Øc th√°i c·∫£m x√∫c kh√°c\",\n",
    "    27: \"T·∫°o 5 hashtag cho b√†i vi·∫øt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "list_of_file_names = result_dict['prefixOnly']\n",
    "incorrect_prefixOnly_list = []\n",
    "for filename in list_of_file_names:\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        content = content.replace(\"json\\nCopy code\\n\", \"\").replace('''\"{''', \"{\").replace('''}\"''', \"}\")\n",
    "        number_pattern = re.compile(r'\\d+')\n",
    "\n",
    "        json_object = json.loads(content, strict=False)\n",
    "        for key, value in json_object['output'].items():\n",
    "            search_text = key[:2] + key[-2:]\n",
    "            match = number_pattern.search(search_text)\n",
    "            if match:\n",
    "                matching_key = int(match.group())\n",
    "                append_text = task_dict[matching_key]\n",
    "            else:\n",
    "                append_text = key\n",
    "                matching_key = next((key for key, value in task_dict.items() if value == append_text), None)\n",
    "\n",
    "            value[\"augmented_instruction\"] = f\"{value['augmented_instruction']}. {append_text}\"\n",
    "\n",
    "        new_filename = filename.replace('.txt', '.json').replace('generated_data_10k_follow_format_task', 'json_10k_prefix_only')\n",
    "        \n",
    "        with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(json_object, json_file, ensure_ascii=False, indent=4)\n",
    "    except:\n",
    "        incorrect_prefixOnly_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1776, 150)"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_file_names),len(incorrect_prefixOnly_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chu·∫©n ho√° key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invalid_list=[]\n",
    "# # directory = \"json_generated_data_10k_follow_format_task\"\n",
    "# directory = \"json_10k_prefix_only\"\n",
    "# number_pattern = re.compile(r'\\d+')\n",
    "\n",
    "# for root, dirs, files in os.walk(directory):\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(root, file)\n",
    "#         if os.path.isfile(file_path):\n",
    "#             with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#                 d = json.load(f)\n",
    "#             try:\n",
    "#                 new_dict = {}\n",
    "#                 for key, value in d['output'].items():\n",
    "# #                     print(f\"***{file_path,key}***\")\n",
    "#                     search_text = key[:2] + key[-2:]\n",
    "#                     match = number_pattern.search(search_text)\n",
    "#                     if match:\n",
    "#                         matching_key = int(match.group())\n",
    "#                         append_text = task_dict[matching_key]\n",
    "#                     else:\n",
    "#                         append_text = key.replace('hastag', 'hashtag').replace('_', ' ')\n",
    "#                         matching_key = next((key for key, value in task_dict.items() if value == append_text), None)\n",
    "\n",
    "#                     new_dict[matching_key] = d['output'][key]\n",
    "#                 new_filename = file_path.replace(directory, 'standard_json_prefix_only_10k')\n",
    "#                 with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "#                     json.dump(new_dict, json_file, ensure_ascii=False, indent=4)\n",
    "#             except:\n",
    "#                 invalid_list.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5414 + 1626 == 7040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None Tao5Hashtag\n",
    "# None DuDoanNhomKhachHangTiemNang\n",
    "# None PhatHienCacThucThe\n",
    "# None Sinhh ra c√¢u h·ªèi thƒÉm d√≤ √Ω ki·∫øn kh√°ch h√†ng\n",
    "# None Sinhh 1 FAQ cho s·∫£n ph·∫©m\n",
    "# None Sinhr ra n·ªôi dung v·ªõi s·∫Øc th√°i c·∫£m x√∫c kh√°c\n",
    "# None analyze_and_summarize_content\n",
    "# None extract_information_with_possible_attributes\n",
    "# None generate_5_possible_comments\n",
    "# None generate_appropriate_extracted_attributes\n",
    "# None classify_content\n",
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhFAQChoSanPham\n",
    "# None SinhCauHoiThamDoYKienKhachHang\n",
    "# None PhanLoaiNoiDung\n",
    "# None PhanTichTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhBaiVietTuongTuSaoThaiKhac\n",
    "# None SinhBinhLuanYDinhMuaHang\n",
    "# None SinhTieuDe\n",
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhFAQChoSanPham\n",
    "# None SinhRaTieuDe\n",
    "# None PhatHienGiaTriThuocTinh\n",
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None PhanLoaiThucThe\n",
    "# None TaoTenChuDe\n",
    "# None TrichXuatThongTinThuocTinh\n",
    "# None PhanTichTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhCauHoiTheoYKienKhachHang\n",
    "# None Tao2CauHoiVaCauTraLoiTuongUng\n",
    "# None SinhFAQChoSanPham\n",
    "# None PhanTichVaTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhFAQChoSanPham\n",
    "# None SinhRaTieuDe\n",
    "# None TrichXuatThongTinNguoiMuaBan\n",
    "# None analyze_and_summarize_content\n",
    "# None extract_information\n",
    "# None generate_faq_for_product\n",
    "# None find_related_products\n",
    "# None classify_content\n",
    "# None PhanTichTomTatNoiDung\n",
    "# None TrichXuatThongTin\n",
    "# None SinhRaCauHoiThamDoYKienKhachHang\n",
    "# None PhanLoaiNoiDung\n",
    "# None PhanLoaiThucThe\n",
    "# None Sinhr ra b√†i vi·∫øt t∆∞∆°ng t·ª± v·ªõi s·∫Øc th√°i kh√°c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = {\n",
    "    \"sample_10k\": \"\"\"B·∫°n l√† m·ªôt AI tr·ª£ l√Ω t·∫°o file JSON, nhi·ªám v·ª• c·ªßa b·∫°n ch·ªâ l√† t·∫°o JSON v√† b·∫°n lu√¥n lu√¥n tr·∫£ v·ªÅ JSON kh√¥ng c√≥ th√™m m√¥ t·∫£ n√†o kh√°c. H√£y d·ª±a v√†o √Ω c·ªßa t·ª´ng task trong task list t·∫°o c√¢u h·ªèi hay y√™u c·∫ßu m·ªõi ƒëa d·∫°ng, t·ª± nhi√™n v√† c√≥ th·ªÉ th·ª±c hi·ªán ƒë·ªëi v·ªõi n·ªôi dung trong INPUT ƒë∆∞·ª£c cung c·∫•p, sau ƒë√≥ t·ª± tr·∫£ l·ªùi.\n",
    "\n",
    "Task list:\n",
    "{tasks}\n",
    "\n",
    "INPUT:\n",
    "{post}\n",
    "\n",
    "Ch√∫ √Ω: Ph·∫£i tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng m·ªôt json file duy nh·∫•t theo format ƒë·∫£m b·∫£o ƒë·∫ßy ƒë·ªß c√°c fields trong ƒë√≥, n·∫øu instruction kh√¥ng ph√π h·ª£p v·ªõi ƒëo·∫°n vƒÉn th√¨ tr·∫£ l·ªùi v·ªõi √Ω kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi trong ƒëo·∫°n vƒÉn. H√£y ch·ªâ tr·∫£ l·ªùi m·ªôt file json duy nh·∫•t, sau ƒë√¢y l√† ƒë·ªãnh d·∫°ng json file c·∫ßn tu√¢n theo trong m·ªçi tr∆∞·ªùng h·ª£p:\n",
    "{{\n",
    "\"task_name\": {{\n",
    "\"augmented_instruction\": \"D·ª±a v√†o √Ω c·ªßa task sau ƒë√≥ t·∫°o c√¢u h·ªèi ho·∫∑c c√¢u l·ªánh v·ªõi t√°c v·ª• ƒë√£ ch·ªçn, h√£y t·∫°o ƒëa d·∫°ng t·ª´ ng·ªØ v√† t·ª± nhi√™n\",\n",
    "\"json_output\": <Tr·∫£ l·ªùi v·ªõi d·∫°ng JSON>,\n",
    "\"text_output\": <Tr·∫£ l·ªùi v·ªõi d·∫°ng text>\n",
    "}},\n",
    "\"task_name\": ...\n",
    "}}\n",
    "\n",
    "Ch√∫ √Ω: H√£y tr·∫£ l·ªùi theo format json tr√™n kh√¥ng th√™m c√°c ph·∫ßn b√™n ngo√†i, kh√¥ng ƒë∆∞·ª£c ph√¢n chia th√†nh json nh·ªè cho m·ªói task. H√£y c·ªë g·∫Øng tr·∫£ l·ªùi v√† tr·∫£ l·ªùi tu√¢n theo format v√† lu√¥n c·ªë g·∫Øng tr·∫£ l·ªùi ƒë·ªß c√°c tasks\"\"\",\n",
    "    \"khang_prompt_1k\" : \"\"\"B·∫°n l√† chuy√™n gia social listening ph√¢n t√≠ch to√†n di·ªán v√† ƒë·∫ßy ƒë·ªß cho b√†i ƒëƒÉng\n",
    "C√°c t√°c v·ª• g·ª£i √Ω nh∆∞ sau: c√¢u ch·ªß ƒë·ªÅ, t√≥m t·∫Øt ng·∫Øn g·ªçn √Ω ch√≠nh, ƒë√°nh gi√° t·ªïng quan th√°i ƒë·ªô, tr√≠ch xu·∫•t th√¥ng tin, li·ªán h·ªá tr·ª±c ti·∫øp, li√™n h·ªá tr·ª±c tuy·∫øn, tr√≠ch xu·∫•t th·ª±c th·ªÉ v√† c√°c thu·ªôc t√≠nh, tr√≠ch xu·∫•t c·∫£m x√∫c theo kh√≠a c·∫°nh, ph√¢n t√≠ch √Ω nghƒ©a ·∫©n d·ª• c√°c emoji, c√°c highlight keywords.\n",
    "L∆∞u √Ω:  ch·ªâ tr·∫£ v·ªÅ json. cu·ªëi json ph√¢n t√≠ch c·∫ßn ƒë∆∞a ra v√†i quan ƒëi·ªÉm tr√°i chi·ªÅu v·ªõi b√†i v√† l·ªùi khuy√™n ƒë·ªëi v·ªõi ng∆∞·ªùi ƒë·ªçc. \n",
    "B√†i Vi·∫øt:\n",
    "{post}\n",
    "\"\"\",\n",
    "    'bao_prompt_2k':  \"\"\"B·∫°n l√† m·ªôt chuy√™n gia social listening. Khi nh·∫≠n m·ªôt b√†i vi·∫øt, h√£y ng·∫´u nhi√™n ch·ªçn m·ªôt s·ªë √≠t t√°c v·ª• t·ª´ danh s√°ch d∆∞·ªõi ƒë√¢y ƒë·ªÉ th·ª±c hi·ªán. Kh√¥ng c·∫ßn th·ª±c hi·ªán t·∫•t c·∫£ c√°c t√°c v·ª•, ch·ªâ c·∫ßn ƒë·∫£m b·∫£o r·∫±ng m·ªói t√°c v·ª• ƒë∆∞·ª£c ch·ªçn th·ª±c hi·ªán m·ªôt c√°ch to√†n di·ªán, ch√≠nh x√°c v√† trung th·ª±c, tr√°nh cung c·∫•p th√¥ng tin sai l·ªách kh√¥ng c√≥ trong b√†i vi·∫øt. C√°c ph·∫£n h·ªìi n√™n s·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng xen k·∫Ω gi·ªØa JSON v√† vƒÉn b·∫£n. Kh√¥ng c·∫ßn th√™m b·∫•t k·ª≥ c√¢u m·ªü ƒë·∫ßu ho·∫∑c k·∫øt th√∫c n√†o kh√°c ngo√†i th√¥ng tin c·∫ßn thi·∫øt cho t·ª´ng t√°c v·ª•. Danh s√°ch c√°c t√°c v·ª• c√≥ th·ªÉ ch·ªçn bao g·ªìm:\n",
    "\n",
    "{tasks}\n",
    "\n",
    "H√£y l·ª±a ch·ªçn ch·ªâ m·ªôt s·ªë √≠t c√°c t√°c v·ª• ƒë·ªÉ th·ª±c hi·ªán v√† xem x√©t l·∫°i c√°c ph·∫£n h·ªìi, ƒë·∫£m b·∫£o s·ª≠ d·ª•ng c·∫£ hai ƒë·ªãnh d·∫°ng JSON v√† vƒÉn b·∫£n. B√†i vi·∫øt c·∫ßn ph√¢n t√≠ch l√†: \n",
    "\n",
    "{post}\n",
    "\"\"\",\n",
    "    'thuc_prompt_100k_post_comment': \"\"\"B√†i vi·∫øt v√†i b√¨nh lu·∫≠n tr√™n m·∫°ng x√£ h·ªôi t∆∞∆°ng ·ª©ng nh∆∞ sau, h√£y ƒë·ªçc v√† th·ª±c hi·ªán nhi·ªám v·ª• b√™n d∆∞·ªõi: \n",
    "{post}\n",
    "Nhi·ªám v·ª•: \n",
    "H√£y th·ª±c hi·ªán c√°c ph√¢n t√≠ch social listening sau: \n",
    "Cho b√†i vi·∫øt: ph√¢n t√≠ch n·ªôi dung ch√≠nh, tr√≠ch xu·∫•t th·ª±c th·ªÉ ch·ªß ƒë·ªÅ c·ªßa b√†i vi·∫øt, t√¨nh c·∫£m c·ªßa b√†i vi·∫øt ?  gi·∫£i th√≠ch emoji b√†i vi·∫øt.\n",
    "\n",
    "Cho b√¨nh lu·∫≠n: gi·∫£i th√≠ch d√†i, r√µ nghƒ©a n·ªôi d·ª•ng b√¨nh lu·∫≠n k√®m ph√¢n lo·∫°i t√≠ch c·ª±c/ti√™u c·ª±c/trung t√≠nh cho t·ª´ng b√¨nh lu·∫≠n v√† ƒë·ªÅ xu·∫•t c√°c emoji cho t·ª´ng b√¨nh lu·∫≠n k√®m √Ω nghƒ©a, tr√≠ch xu·∫•t ƒë∆°n h√†ng th√†nh d·∫°ng json {{m·∫∑t h√†ng, s·ªë l∆∞·ª£ng,  gi√° ti·ªÅn, thu·ªôc t√≠nh kh√°c..., sdt}}, cu·ªëi c√πng h√£y t·∫°o nh·∫≠n x√©t quan ƒëi·ªÉm chung c·ªßa c√°c b√¨nh lu·∫≠n v·ªÅ b√†i vi·∫øt. L∆∞u √Ω: Ch·ªâ tr·∫£ v·ªÅ json.\"\"\",\n",
    "    'thuc_prompt_100k_post_comment_v2': \"\"\"B√†i vi·∫øt v√†i b√¨nh lu·∫≠n tr√™n m·∫°ng x√£ h·ªôi t∆∞∆°ng ·ª©ng nh∆∞ sau, h√£y ƒë·ªçc v√† th·ª±c hi·ªán nhi·ªám v·ª• b√™n d∆∞·ªõi: \n",
    "{post}\n",
    "Nhi·ªám v·ª•: \n",
    "H√£y th·ª±c hi·ªán t√°c ph√¢n t√≠ch social listening sau: \n",
    "ƒê·ªëi v·ªõi b√†i vi·∫øt: cho bi·∫øt n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt.\n",
    "ƒê·ªëi v·ªõi b√¨nh lu·∫≠n: h√£y ch·ªâ ra √Ω ƒë·ªì c·ªßa t·ª´ng b√¨nh lu·∫≠n, h√£y t·∫°o c√¢u tr·∫£ l·ªùi cho t·ª´ng b√¨nh lu·∫≠n k√®m emoji ph√π h·ª£p d·ª±a v√†o ng·ªØ c·∫£nh b√†i vi·∫øt\n",
    "L∆∞u √Ω tr·∫£ v·ªÅ JSON.\"\"\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sample_10k', 'khang_prompt_1k', 'bao_prompt_2k', 'thuc_prompt_100k_post_comment', 'thuc_prompt_100k_post_comment_v2'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYSTEM_PROMPT.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_TASK = {\n",
    "    1: \"Ph√¢n t√≠ch v√† t√≥m t·∫Øt n·ªôi dung\",\n",
    "    2: \"Tr√≠ch xu·∫•t th√¥ng tin v·ªõi c√°c thu·ªôc t√≠nh c√≥ th·ªÉ c√≥ trong b√†i\",\n",
    "    3: \"Sinh ra ti√™u ƒë·ªÅ\",\n",
    "    4: \"T·∫°o t√™n ch·ªß ƒë·ªÅ\",\n",
    "    5: \"T·∫°o 5 b√¨nh lu·∫≠n c√≥ th·ªÉ c√≥\",\n",
    "    6: \"Ph√¢n lo·∫°i n·ªôi dung\",\n",
    "    7: \"Ph√°t hi·ªán c√°c th·ª±c th·ªÉ\",\n",
    "    8: \"Tr√≠ch xu·∫•t th√¥ng tin v·ªõi c√°c thu·ªôc t√≠nh c√≥ th·ªÉ c√≥ trong b√†i\",\n",
    "    9: \"T√¨m s·∫£n ph·∫©m c√≥ th·ªÉ li√™n quan\",\n",
    "    10: \"T·∫°o ƒë√°nh gi√°\",\n",
    "    11: \"T·∫°o 2 c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi t∆∞∆°ng ·ª©ng\",\n",
    "    12: \"Sinh ra b√¨nh lu·∫≠n √Ω ƒë·ªãnh mua h√†ng\",\n",
    "    13: \"Ph√¢n lo·∫°i th·ª±c th·ªÉ\",\n",
    "    14: \"D·ª± ƒëo√°n nh√≥m kh√°ch h√†ng ti·ªÅm nƒÉng\",\n",
    "    15: \"Ph√°t hi·ªán gi√° tr·ªã thu·ªôc t√≠nh\",\n",
    "    16: \"Tr√≠ch xu·∫•t th√¥ng tin ng∆∞·ªùi mua ho·∫∑c b√°n\",\n",
    "    17: \"Sinh ra b√†i vi·∫øt t∆∞∆°ng t·ª± v·ªõi s·∫Øc th√°i kh√°c\",\n",
    "    18: \"Sinh ra thu·ªôc t√≠nh c√≥ th·ªÉ tr√≠ch xu·∫•t ph√π h·ª£p\",\n",
    "    19: \"S·ª≠a l·ªói ch√≠nh t·∫£ v√† vi·∫øt t·∫Øt\",\n",
    "    20: \"Ph√¢n t√≠ch c√°c emoji\",\n",
    "    21: \"Ph√¢n t√≠ch th√°i ƒë·ªô c·∫£m x√∫c\",\n",
    "    22: \"Sinh ra n·ªôi dung v·ªõi s·∫Øc th√°i c·∫£m x√∫c kh√°c\",\n",
    "    23: \"Sinh ra √Ω t∆∞·ªüng qu·∫£ng c√°o s·∫£n ph·∫©m\",\n",
    "    24: \"Sinh ra c√¢u h·ªèi thƒÉm d√≤ √Ω ki·∫øn kh√°ch h√†ng\",\n",
    "    25: \"Sinh 1 FAQ cho s·∫£n ph·∫©m\",\n",
    "    26: \"T·∫°o 5 b√¨nh lu·∫≠n c√≥ th·ªÉ c√≥ s·∫Øc th√°i c·∫£m x√∫c kh√°c\",\n",
    "    27: \"T·∫°o 5 hashtag cho b√†i vi·∫øt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "df = pd.read_csv('data_source/sample1_10k_vie_fb_post_rows_47k_idx_1000-1002_char-length_500-1100.csv',  error_bad_lines=False, sep='\\t')\n",
    "all_posts = df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_dataset/cleaned_dataset/standard_json_prefix_only_10k'\n",
    "oputput_directory = '231227_instruction_social_postComment_7k_post_10k/instructions_social_sample10k_1k6_prefix_only_basic_prompt'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = json.load(f)\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['sample_10k']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            dict_iio['output'] = gpt_output\n",
    "            new_filename = file_path = os.path.join(oputput_directory, file)\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_dataset/cleaned_dataset/standard_json_valid_10k'\n",
    "oputput_directory = '231227_instruction_social_postComment_7k_post_10k/instructions_social_sample10k_5k4_valid_basic_prompt'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        if 'checkpoint' in file:\n",
    "            continue\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = json.load(f)\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['sample_10k']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            dict_iio['output'] = gpt_output\n",
    "            new_filename = file_path = os.path.join(oputput_directory, file)\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KHANG 1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37837, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37837"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_posts_df = pd.read_csv('data_source/vie_fb_post_rows_47k_idx_1000-1002_char-length_500-1100.csv', error_bad_lines=False, sep='\\t')\n",
    "all_posts_df = all_posts_df.drop(index=range(10000))\n",
    "all_posts_df = all_posts_df.dropna()\n",
    "print(all_posts_df.shape)\n",
    "all_posts = all_posts_df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_231226_17k_dataset/cleaned_dataset/cleaned_txt_instructions_social_1k_post_khang_prompt'\n",
    "oputput_directory = '231227_instruction_social_7k_post_comment_10k_post/instructions_social_1k_idx0_post36k_khang_prompt'\n",
    "temp = []\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = f.read()\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['khang_prompt_1k']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "            begin_char = gpt_output[10]\n",
    "            start = 10 if begin_char == '{' else 11\n",
    "            if begin_char == '{':\n",
    "                start = 10\n",
    "                end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "            else:\n",
    "                start = 11\n",
    "                end = gpt_output.rfind('\"')\n",
    "            dict_iio['output'] = gpt_output[start:end]\n",
    "            newfile = f'{idx}.json'\n",
    "            new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAO 36K (2k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37837, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37837"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_posts_df = pd.read_csv('data_source/vie_fb_post_rows_47k_idx_1000-1002_char-length_500-1100.csv', error_bad_lines=False, sep='\\t')\n",
    "all_posts_df = all_posts_df.drop(index=range(10000))\n",
    "all_posts_df = all_posts_df.dropna()\n",
    "print(all_posts_df.shape)\n",
    "all_posts = all_posts_df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_231226_17k_dataset/cleaned_dataset/cleaned_txt_generated_36k_10100_json_text_prompt'\n",
    "oputput_directory = '231227_instruction_social_7k_post_comment_10k_post/instructions_social_2k_idx1k_post36k_bao_prompt'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = f.read()\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['bao_prompt_2k'].replace('[no closing notes] ', '')\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "            begin_char = gpt_output[10]\n",
    "            start = 10 if begin_char == '{' else 11\n",
    "            if begin_char == '{':\n",
    "                start = 10\n",
    "                end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "            else:\n",
    "                start = 11\n",
    "                end = gpt_output.rfind('\"')\n",
    "            dict_iio['output'] = gpt_output[start:end].replace('[no closing notes]', '')\n",
    "            newfile = f'{idx}.json'\n",
    "            new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THUC 10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111443"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "all_posts_df = pd.read_csv('raw_data_source/facebook_100k_post_comment.csv')\n",
    "all_posts_df = all_posts_df.dropna()\n",
    "all_posts = all_posts_df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_231226_17k_dataset/cleaned_dataset/cleaned_generated_100k_facebook_post_comment_thuc_prompt'\n",
    "oputput_directory = '231227_instruction_social_7k_post_comment_10k_post/instructions_social_7k_idx0_postCommnet111k_thuc_prompt_v1'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = f.read()\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['thuc_prompt_100k_post_comment']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "            begin_char = gpt_output[10]\n",
    "            start = 10 if begin_char == '{' else 11\n",
    "            if begin_char == '{':\n",
    "                start = 10\n",
    "                end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "            else:\n",
    "                start = 11\n",
    "                end = gpt_output.rfind('\"')\n",
    "            dict_iio['output'] = gpt_output[start:end]\n",
    "            newfile = f'{idx}.json'\n",
    "            new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "            \n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thuc prompt v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856\n",
      "{\"output\":\"json\n",
      "Copy code\n",
      "{\n",
      "  \"bai_viet\": {\n",
      "    \"noi_dung_chinh\": \"M·∫π Trang ƒëang c√≥ file s√°ch 'K·ª∑ lu·∫≠t t√≠ch c·ª±c' do ti·∫øn sƒ© L√™ VƒÉn H·∫£o - Vi·ªán t√¢m l√Ω h·ªçc v√† t·ªï ch·ª©c Plan t·∫°i Vi·ªát Nam ch·ªãu tr√°ch nhi·ªám n·ªôi dung. B·ªô t√†i li·ªáu n√†y s·∫Ω gi√∫p c√°c b·∫≠c ph·ª• huynh, gi√°o vi√™n hi·ªÉu r√µ h∆°n v·ªÅ tr·∫ª em, t√¢m l√Ω l·ª©a tu·ªïi, t√°c d·ª•ng ti√™u c·ª±c c·ªßa vi·ªác tr·ª´ng ph·∫°t tr·∫ª em v√† gi√∫p ph·ª• huynh, gi√°o vi√™n trang b·ªã ki·∫øn th·ª©c, k·ªπ nƒÉng nh·∫±m gi√°o d·ª•c tr·∫ª em m·ªôt c√°ch t√≠ch c·ª±c-hi·ªáu qu·∫£. M·∫∏ N√ÄO MU·ªêN NH·∫¨N FILE H√ÉY COMMENT EMAIL PH√çA D∆Ø·ªöI, M·∫∏ TRANG S·∫º G·ª¨I CHO C√ÅC M·∫∏ NHA. Y√™u th∆∞∆°ng th·∫≠t nhieuuuuuuuuuu..!ü•∞\"\n",
      "  },\n",
      "  \"binh_luan\": [\n",
      "    {\n",
      "      \"noi_dung\": \"B√πi Di·ªáp d·∫° em ƒë√£ g·ª≠i nha . M·∫π check mail gi√∫p em v·ªõi nh√© üíóüíóüíó\",\n",
      "      \"y_do\": \"Y√™u c·∫ßu nh·∫≠n file v√† th√¥ng b√°o ƒë√£ g·ª≠i email, mong ƒë·ª£i s·ª± h·ªó tr·ª£ t·ª´ M·∫π Trang\",\n",
      "      \"phan_hoi\": \"ƒê√£ nh·∫≠n, m·∫π B√πi Di·ªáp! C·∫£m ∆°n em ƒë√£ g·ª≠i, m·∫π Trang s·∫Ω ki·ªÉm tra v√† g·ª≠i file cho em ngay. üíñ\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"M·∫π trang cho m√¨nh xin v·ªõi nha. tamyhuongktt1709@gmail.com C·∫£m ∆°n C√¥ nhi·ªÅu\",\n",
      "      \"y_do\": \"Y√™u c·∫ßu nh·∫≠n file v√† cung c·∫•p email ƒë·ªÉ nh·∫≠n\",\n",
      "      \"phai_tra_loi\": \"ƒê√£ nh·∫≠n, m·∫π T√¢m √ù! M·∫π Trang s·∫Ω g·ª≠i file ƒë·∫øn ƒë·ªãa ch·ªâ email c·ªßa m·∫π. C·∫£m ∆°n m·∫π ƒë√£ quan t√¢m! üíï\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"G·ª≠i d√πm v·ªõi nha c√¥ Trang. Nguyenphong0210@gmail.com\",\n",
      "      \"y_do\": \"Y√™u c·∫ßu nh·∫≠n file v√† cung c·∫•p email ƒë·ªÉ nh·∫≠n\",\n",
      "      \"phai_tra_loi\": \"ƒê√£ g·ª≠i, m·∫π Phong Nguyen! M·∫π Trang s·∫Ω ch·∫Øc ch·∫Øn g·ª≠i file ƒë·∫øn ƒë·ªãa ch·ªâ email c·ªßa m·∫π. C·∫£m ∆°n m·∫π! üì©\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Phong Nguyen d·∫° em ƒë√£ g·ª≠i s√°ch ·∫° . Ba check email gi√∫p em v·ªõi nh√© !\",\n",
      "      \"y_do\": \"Th√¥ng b√°o ƒë√£ g·ª≠i file v√† y√™u c·∫ßu ki·ªÉm tra email\",\n",
      "      \"phai_tra_loi\": \"ƒê√£ nh·∫≠n, em Phong Nguyen! Ba s·∫Ω ki·ªÉm tra email v√† th√¥ng b√°o ngay cho em. C·∫£m ∆°n em ƒë√£ chia s·∫ª! üëç\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Sakura Sakura d·∫° em ƒë√£ g·ª≠i r·ªìi ·∫° . M·∫π check mail gi√∫p em nh√© üíóüíóüíó\",\n",
      "      \"y_do\": \"Th√¥ng b√°o ƒë√£ g·ª≠i file v√† y√™u c·∫ßu ki·ªÉm tra email\",\n",
      "      \"phai_tra_loi\": \"C·∫£m ∆°n em Sakura Sakura! M·∫π s·∫Ω ki·ªÉm tra email v√† g·ª≠i file ƒë·∫øn em ngay. üíñ\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Hi·ªÅn Hi·ªÅn d·∫° em ƒë√£ g·ª≠i s√°ch ·∫° . C check mail gi√∫p em v·ªõi nh√© üíóüíó\",\n",
      "      \"y_do\": \"Th√¥ng b√°o ƒë√£ g·ª≠i file v√† y√™u c·∫ßu ki·ªÉm tra email\",\n",
      "      \"phai_tra_loi\": \"C·∫£m ∆°n em Hi·ªÅn Hi·ªÅn! C s·∫Ω ki·ªÉm tra email v√† g·ª≠i file cho em ngay. üíï\"\n",
      "    },\n",
      "    {\n",
      "      \"noi_dung\": \"Hu·ª≥nh L√†nh d·∫° em g·ª≠i lu√¥n ·∫° hihi\",\n",
      "      \"y_do\": \"Th√¥ng b√°o ƒë√£ g·ª≠i file\",\n",
      "      \"phai_tra_loi\": \"C·∫£m ∆°n em Hu·ª≥nh L√†nh! M·∫π Trang s·∫Ω ki·ªÉm tra v√† g·ª≠i file ƒë·∫øn em ngay. Hihi üíñ\"\n",
      "    }\n",
      "  ]\n",
      "}\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i = random.randint(0, len(result_dict['valid']))\n",
    "file_path = result_dict['valid'][i]\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "print(i)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111443"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_posts_df = pd.read_csv('data_source/facebook_100k_post_comment.csv')\n",
    "all_posts_df = all_posts_df.dropna()\n",
    "all_posts = all_posts_df['text'].tolist()\n",
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idx_11366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = 'backup_dataset/cleaned_dataset/cleaned_generated_idx_11366_100k_facebook_post_comment_thuc_prompt_ver2'\n",
    "oputput_directory = '231227_instruction_social_postComment_7k_post_10k/instructions_social_postCommnet111k_1k5_idx11366_thuc_prompt_v2'\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            dict_iio = {}\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                gpt_output = f.read()\n",
    "            dict_iio['instruction'] = SYSTEM_PROMPT['thuc_prompt_100k_post_comment_v2']\n",
    "            idx = int(file.split('.')[0])\n",
    "            dict_iio['input'] = all_posts[idx]\n",
    "            gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "            begin_char = gpt_output[10]\n",
    "            start = 10 if begin_char == '{' else 11\n",
    "            if begin_char == '{':\n",
    "                start = 10\n",
    "                end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "            else:\n",
    "                start = 11\n",
    "                end = gpt_output.rfind('\"')\n",
    "            dict_iio['output'] = gpt_output[start:end]\n",
    "            newfile = f'{idx}.json'\n",
    "            new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "\n",
    "            with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idx_16583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"output\":\"JSON cho ph·∫£n h·ªìi:\\njson\\nCopy code\\n{\\n  \"bai_viet\": {\\n    \"ngay_dang\": \"14.03.2022\",\\n    \"noi_dung\": \"C√¥ g√°i, ch√∫c cho em mau qu√™n ƒëi chuy·ªán c≈©, tha th·ª© cho nh·ªØng ng∆∞·ªùi t·ª´ng l√†m t·ªïn th∆∞∆°ng em. B·ªüi em bi·∫øt kh√¥ng? Khi em c√≤n n·∫∑ng l√≤ng v·ªÅ nh·ªØng ƒëi·ªÅu ·∫•y, em s·∫Ω ch·∫≥ng th·ªÉ s·ªëng m·ªôt cu·ªôc ƒë·ªùi b√¨nh b√¨nh y√™n nh∆∞ em mong mu·ªën. Ch√∫c em m·ªói ng√†y ƒë·ªÅu c∆∞·ªùi th·∫≠t t∆∞∆°i, d·∫´u cho h√¥m ·∫•y c√≥ g·∫∑p g·ª° ai, g·∫∑p chuy·ªán bu·ªìn, nh·ªØng vi·ªác kh√¥ng vi√™n m√£n ƒëi chƒÉng n·ªØa, xin em h√£y nh·ªõ.. m·ªçi vi·ªác ƒë·ªÅu s·∫Ω v∆∞·ª£t qua ƒë∆∞·ª£c - N·∫øu em tin ch√≠nh em. Ch√∫c em c·∫£ n·ª≠a ƒë·ªùi v·ªÅ sau, s·ªëng m·ªôt cu·ªôc ƒë·ªùi th·∫≠t ki√™u h√£nh. Y√™u ƒë√∫ng ng∆∞·ªùi..\"\\n  },\\n  \"binh_luan\": [\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"ü•∞ ch√∫c em m·ªôt ƒë·ªùi an nhi√™n vui v·∫ª h·∫°nh ph√∫c b·ªè qua  m·ªçi chuy·ªán l√†m em bu·ªìn t·ªßi, y√™u th∆∞∆°ng ch√≠nh m√¨nh . R·ªìi m·ªçi chuy·ªán s·∫Ω qua th√¥i Ch√∫c em t·∫•t c·∫£ü•∞üçÄ\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª l·ªùi ch√∫c √Ω nghƒ©a! üå∏ Em s·∫Ω c·ªë g·∫Øng v∆∞·ª£t qua m·ªçi kh√≥ khƒÉn v√† s·ªëng h·∫°nh ph√∫c. üåà\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"15.03.2022\",\\n      \"noi_dung\": \"#S 15/3/2022 ch√∫c e qu√£g ƒë∆∞·ªùng sau n√†y ƒë·∫ßy h·∫°nh ph√∫c v√† an y√™n..\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n nhi·ªÅu! Mong r·∫±ng t∆∞∆°ng lai c·ªßa em s·∫Ω ƒë·∫ßy ·∫Øp h·∫°nh ph√∫c v√† an y√™n nh∆∞ b·∫°n ƒë√£ ch√∫c. üåü\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Chuc em co gai cua toi mot minh van that hanh phuc üçÄ\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n ƒë√£ ch√∫c em! Hy v·ªçng em s·∫Ω t√¨m th·∫•y h·∫°nh ph√∫c v√† ni·ªÅm vui trong cu·ªôc s·ªëng. üåπ\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Ch√∫c em lu√¥n an y√™n vui v·∫ª\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n! An y√™n v√† vui v·∫ª l√† ƒëi·ªÅu em ƒëang h∆∞·ªõng ƒë·∫øn. üíñ\"\\n    }\\n  ]\\n}\\nƒê√¢y l√† m·ªôt bi·ªÉu di·ªÖn JSON c·ªßa b√†i vi·∫øt v√† c√°c b√¨nh lu·∫≠n tr√™n m·∫°ng x√£ h·ªôi, bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt v√† √Ω ƒë·ªì c·ªßa t·ª´ng b√¨nh lu·∫≠n c√πng v·ªõi c√¢u tr·∫£ l·ªùi v√† emoji ph√π h·ª£p.\"}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open ('generated_idx_16583_100k_facebook_post_comment_thuc_prompt_ver2/20735.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 7783\n",
      "Valid : 6674\n",
      "PrefixOnly: 0\n",
      "Empty:1109\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid\\_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 1000:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            if search_string in content:\n",
    "#                             if content.count(search_string) == 5:\n",
    "                                prefixOnly_files.append(file_path)\n",
    "                            else:\n",
    "                                valid_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_to_search = 'generated_idx_16583_100k_facebook_post_comment_thuc_prompt_ver2'\n",
    "search_string = '{\"output\":\"Message: invalid session id'\n",
    "\n",
    "result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "\n",
    "print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "Valid : {len(result_dict['valid']) }\n",
    "PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20735"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = result_dict['valid'][1]\n",
    "int(file_path.split('/')[-1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"output\":\"JSON cho ph·∫£n h·ªìi:\\njson\\nCopy code\\n{\\n  \"bai_viet\": {\\n    \"ngay_dang\": \"14.03.2022\",\\n    \"noi_dung\": \"C√¥ g√°i, ch√∫c cho em mau qu√™n ƒëi chuy·ªán c≈©, tha th·ª© cho nh·ªØng ng∆∞·ªùi t·ª´ng l√†m t·ªïn th∆∞∆°ng em. B·ªüi em bi·∫øt kh√¥ng? Khi em c√≤n n·∫∑ng l√≤ng v·ªÅ nh·ªØng ƒëi·ªÅu ·∫•y, em s·∫Ω ch·∫≥ng th·ªÉ s·ªëng m·ªôt cu·ªôc ƒë·ªùi b√¨nh b√¨nh y√™n nh∆∞ em mong mu·ªën. Ch√∫c em m·ªói ng√†y ƒë·ªÅu c∆∞·ªùi th·∫≠t t∆∞∆°i, d·∫´u cho h√¥m ·∫•y c√≥ g·∫∑p g·ª° ai, g·∫∑p chuy·ªán bu·ªìn, nh·ªØng vi·ªác kh√¥ng vi√™n m√£n ƒëi chƒÉng n·ªØa, xin em h√£y nh·ªõ.. m·ªçi vi·ªác ƒë·ªÅu s·∫Ω v∆∞·ª£t qua ƒë∆∞·ª£c - N·∫øu em tin ch√≠nh em. Ch√∫c em c·∫£ n·ª≠a ƒë·ªùi v·ªÅ sau, s·ªëng m·ªôt cu·ªôc ƒë·ªùi th·∫≠t ki√™u h√£nh. Y√™u ƒë√∫ng ng∆∞·ªùi..\"\\n  },\\n  \"binh_luan\": [\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"ü•∞ ch√∫c em m·ªôt ƒë·ªùi an nhi√™n vui v·∫ª h·∫°nh ph√∫c b·ªè qua  m·ªçi chuy·ªán l√†m em bu·ªìn t·ªßi, y√™u th∆∞∆°ng ch√≠nh m√¨nh . R·ªìi m·ªçi chuy·ªán s·∫Ω qua th√¥i Ch√∫c em t·∫•t c·∫£ü•∞üçÄ\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª l·ªùi ch√∫c √Ω nghƒ©a! üå∏ Em s·∫Ω c·ªë g·∫Øng v∆∞·ª£t qua m·ªçi kh√≥ khƒÉn v√† s·ªëng h·∫°nh ph√∫c. üåà\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"15.03.2022\",\\n      \"noi_dung\": \"#S 15/3/2022 ch√∫c e qu√£g ƒë∆∞·ªùng sau n√†y ƒë·∫ßy h·∫°nh ph√∫c v√† an y√™n..\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n nhi·ªÅu! Mong r·∫±ng t∆∞∆°ng lai c·ªßa em s·∫Ω ƒë·∫ßy ·∫Øp h·∫°nh ph√∫c v√† an y√™n nh∆∞ b·∫°n ƒë√£ ch√∫c. üåü\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Chuc em co gai cua toi mot minh van that hanh phuc üçÄ\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n ƒë√£ ch√∫c em! Hy v·ªçng em s·∫Ω t√¨m th·∫•y h·∫°nh ph√∫c v√† ni·ªÅm vui trong cu·ªôc s·ªëng. üåπ\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Ch√∫c em lu√¥n an y√™n vui v·∫ª\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n! An y√™n v√† vui v·∫ª l√† ƒëi·ªÅu em ƒëang h∆∞·ªõng ƒë·∫øn. üíñ\"\\n    }\\n  ]\\n}\\nƒê√¢y l√† m·ªôt bi·ªÉu di·ªÖn JSON c·ªßa b√†i vi·∫øt v√† c√°c b√¨nh lu·∫≠n tr√™n m·∫°ng x√£ h·ªôi, bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt v√† √Ω ƒë·ªì c·ªßa t·ª´ng b√¨nh lu·∫≠n c√πng v·ªõi c√¢u tr·∫£ l·ªùi v√† emoji ph√π h·ª£p.\"}\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    gpt_output = f.read()\n",
    "    f.close()\n",
    "gpt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"output\":\"JSON cho ph·∫£n h·ªìi:\\n{\\n  \"bai_viet\": {\\n    \"ngay_dang\": \"14.03.2022\",\\n    \"noi_dung\": \"C√¥ g√°i, ch√∫c cho em mau qu√™n ƒëi chuy·ªán c≈©, tha th·ª© cho nh·ªØng ng∆∞·ªùi t·ª´ng l√†m t·ªïn th∆∞∆°ng em. B·ªüi em bi·∫øt kh√¥ng? Khi em c√≤n n·∫∑ng l√≤ng v·ªÅ nh·ªØng ƒëi·ªÅu ·∫•y, em s·∫Ω ch·∫≥ng th·ªÉ s·ªëng m·ªôt cu·ªôc ƒë·ªùi b√¨nh b√¨nh y√™n nh∆∞ em mong mu·ªën. Ch√∫c em m·ªói ng√†y ƒë·ªÅu c∆∞·ªùi th·∫≠t t∆∞∆°i, d·∫´u cho h√¥m ·∫•y c√≥ g·∫∑p g·ª° ai, g·∫∑p chuy·ªán bu·ªìn, nh·ªØng vi·ªác kh√¥ng vi√™n m√£n ƒëi chƒÉng n·ªØa, xin em h√£y nh·ªõ.. m·ªçi vi·ªác ƒë·ªÅu s·∫Ω v∆∞·ª£t qua ƒë∆∞·ª£c - N·∫øu em tin ch√≠nh em. Ch√∫c em c·∫£ n·ª≠a ƒë·ªùi v·ªÅ sau, s·ªëng m·ªôt cu·ªôc ƒë·ªùi th·∫≠t ki√™u h√£nh. Y√™u ƒë√∫ng ng∆∞·ªùi..\"\\n  },\\n  \"binh_luan\": [\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"ü•∞ ch√∫c em m·ªôt ƒë·ªùi an nhi√™n vui v·∫ª h·∫°nh ph√∫c b·ªè qua  m·ªçi chuy·ªán l√†m em bu·ªìn t·ªßi, y√™u th∆∞∆°ng ch√≠nh m√¨nh . R·ªìi m·ªçi chuy·ªán s·∫Ω qua th√¥i Ch√∫c em t·∫•t c·∫£ü•∞üçÄ\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª l·ªùi ch√∫c √Ω nghƒ©a! üå∏ Em s·∫Ω c·ªë g·∫Øng v∆∞·ª£t qua m·ªçi kh√≥ khƒÉn v√† s·ªëng h·∫°nh ph√∫c. üåà\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"15.03.2022\",\\n      \"noi_dung\": \"#S 15/3/2022 ch√∫c e qu√£g ƒë∆∞·ªùng sau n√†y ƒë·∫ßy h·∫°nh ph√∫c v√† an y√™n..\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n nhi·ªÅu! Mong r·∫±ng t∆∞∆°ng lai c·ªßa em s·∫Ω ƒë·∫ßy ·∫Øp h·∫°nh ph√∫c v√† an y√™n nh∆∞ b·∫°n ƒë√£ ch√∫c. üåü\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Chuc em co gai cua toi mot minh van that hanh phuc üçÄ\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n ƒë√£ ch√∫c em! Hy v·ªçng em s·∫Ω t√¨m th·∫•y h·∫°nh ph√∫c v√† ni·ªÅm vui trong cu·ªôc s·ªëng. üåπ\"\\n    },\\n    {\\n      \"ngay_binh_luan\": \"14.03.2022\",\\n      \"noi_dung\": \"Ch√∫c em lu√¥n an y√™n vui v·∫ª\",\\n      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n! An y√™n v√† vui v·∫ª l√† ƒëi·ªÅu em ƒëang h∆∞·ªõng ƒë·∫øn. üíñ\"\\n    }\\n  ]\\n}\\nƒê√¢y l√† m·ªôt bi·ªÉu di·ªÖn JSON c·ªßa b√†i vi·∫øt v√† c√°c b√¨nh lu·∫≠n tr√™n m·∫°ng x√£ h·ªôi, bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt v√† √Ω ƒë·ªì c·ªßa t·ª´ng b√¨nh lu·∫≠n c√πng v·ªõi c√¢u tr·∫£ l·ªùi v√† emoji ph√π h·ª£p.\"}\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "gpt_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON cho ph·∫£n h·ªìi:\n",
      "{\n",
      "  \"bai_viet\": {\n",
      "    \"ngay_dang\": \"14.03.2022\",\n",
      "    \"noi_dung\": \"C√¥ g√°i, ch√∫c cho em mau qu√™n ƒëi chuy·ªán c≈©, tha th·ª© cho nh·ªØng ng∆∞·ªùi t·ª´ng l√†m t·ªïn th∆∞∆°ng em. B·ªüi em bi·∫øt kh√¥ng? Khi em c√≤n n·∫∑ng l√≤ng v·ªÅ nh·ªØng ƒëi·ªÅu ·∫•y, em s·∫Ω ch·∫≥ng th·ªÉ s·ªëng m·ªôt cu·ªôc ƒë·ªùi b√¨nh b√¨nh y√™n nh∆∞ em mong mu·ªën. Ch√∫c em m·ªói ng√†y ƒë·ªÅu c∆∞·ªùi th·∫≠t t∆∞∆°i, d·∫´u cho h√¥m ·∫•y c√≥ g·∫∑p g·ª° ai, g·∫∑p chuy·ªán bu·ªìn, nh·ªØng vi·ªác kh√¥ng vi√™n m√£n ƒëi chƒÉng n·ªØa, xin em h√£y nh·ªõ.. m·ªçi vi·ªác ƒë·ªÅu s·∫Ω v∆∞·ª£t qua ƒë∆∞·ª£c - N·∫øu em tin ch√≠nh em. Ch√∫c em c·∫£ n·ª≠a ƒë·ªùi v·ªÅ sau, s·ªëng m·ªôt cu·ªôc ƒë·ªùi th·∫≠t ki√™u h√£nh. Y√™u ƒë√∫ng ng∆∞·ªùi..\"\n",
      "  },\n",
      "  \"binh_luan\": [\n",
      "    {\n",
      "      \"ngay_binh_luan\": \"14.03.2022\",\n",
      "      \"noi_dung\": \"ü•∞ ch√∫c em m·ªôt ƒë·ªùi an nhi√™n vui v·∫ª h·∫°nh ph√∫c b·ªè qua  m·ªçi chuy·ªán l√†m em bu·ªìn t·ªßi, y√™u th∆∞∆°ng ch√≠nh m√¨nh . R·ªìi m·ªçi chuy·ªán s·∫Ω qua th√¥i Ch√∫c em t·∫•t c·∫£ü•∞üçÄ\",\n",
      "      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n ƒë√£ chia s·∫ª l·ªùi ch√∫c √Ω nghƒ©a! üå∏ Em s·∫Ω c·ªë g·∫Øng v∆∞·ª£t qua m·ªçi kh√≥ khƒÉn v√† s·ªëng h·∫°nh ph√∫c. üåà\"\n",
      "    },\n",
      "    {\n",
      "      \"ngay_binh_luan\": \"15.03.2022\",\n",
      "      \"noi_dung\": \"#S 15/3/2022 ch√∫c e qu√£g ƒë∆∞·ªùng sau n√†y ƒë·∫ßy h·∫°nh ph√∫c v√† an y√™n..\",\n",
      "      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n nhi·ªÅu! Mong r·∫±ng t∆∞∆°ng lai c·ªßa em s·∫Ω ƒë·∫ßy ·∫Øp h·∫°nh ph√∫c v√† an y√™n nh∆∞ b·∫°n ƒë√£ ch√∫c. üåü\"\n",
      "    },\n",
      "    {\n",
      "      \"ngay_binh_luan\": \"14.03.2022\",\n",
      "      \"noi_dung\": \"Chuc em co gai cua toi mot minh van that hanh phuc üçÄ\",\n",
      "      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n ƒë√£ ch√∫c em! Hy v·ªçng em s·∫Ω t√¨m th·∫•y h·∫°nh ph√∫c v√† ni·ªÅm vui trong cu·ªôc s·ªëng. üåπ\"\n",
      "    },\n",
      "    {\n",
      "      \"ngay_binh_luan\": \"14.03.2022\",\n",
      "      \"noi_dung\": \"Ch√∫c em lu√¥n an y√™n vui v·∫ª\",\n",
      "      \"phan_hoi\": \"C·∫£m ∆°n b·∫°n! An y√™n v√† vui v·∫ª l√† ƒëi·ªÅu em ƒëang h∆∞·ªõng ƒë·∫øn. üíñ\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "ƒê√¢y l√† m·ªôt bi·ªÉu di·ªÖn JSON c·ªßa b√†i vi·∫øt v√† c√°c b√¨nh lu·∫≠n tr√™n m·∫°ng x√£ h·ªôi, bao g·ªìm n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt v√† √Ω ƒë·ªì c·ªßa t·ª´ng b√¨nh lu·∫≠n c√πng v·ªõi c√¢u tr·∫£ l·ªùi v√† emoji ph√π h·ª£p.\n"
     ]
    }
   ],
   "source": [
    "begin_char = gpt_output[10]\n",
    "start = 10 if begin_char == '{' else 11\n",
    "if begin_char == '{':\n",
    "    start = 10\n",
    "    end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "else:\n",
    "    start = 11\n",
    "    end = gpt_output.rfind('\"')\n",
    "print(gpt_output[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6674"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_dict['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input_directory = result_dict['valid']\n",
    "oputput_directory = 'instruction_social_data/instructions_social_postCommnet111k_6k7_idx16583_thuc_prompt_v2'\n",
    "for file_path in input_directory:\n",
    "    if os.path.isfile(file_path):\n",
    "        dict_iio = {}\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            gpt_output = f.read()\n",
    "            f.close()\n",
    "\n",
    "        dict_iio['instruction'] = SYSTEM_PROMPT['thuc_prompt_100k_post_comment_v2']\n",
    "        idx = int(file_path.split('/')[-1].split('.')[0])\n",
    "        dict_iio['input'] = all_posts[idx]\n",
    "        gpt_output = gpt_output.replace(\"json\\nCopy code\\n\", \"\")\n",
    "        begin_char = gpt_output[10]\n",
    "        start = 10 if begin_char == '{' else 11\n",
    "        if begin_char == '{':\n",
    "            start = 10\n",
    "            end = gpt_output.rfind('}', 0, gpt_output.rfind('}'))+1\n",
    "        else:\n",
    "            start = 11\n",
    "            end = gpt_output.rfind('\"')\n",
    "        dict_iio['output'] = gpt_output[start:end]\n",
    "        newfile = f'{idx}.json'\n",
    "        new_filename = file_path = os.path.join(oputput_directory, newfile)\n",
    "        print(new_filename)\n",
    "        print(dict_iio)\n",
    "        with open(f'{new_filename}', 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(dict_iio, json_file, ensure_ascii=False, indent=4)\n",
    "            json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total: 7783\n",
    "# Valid : 6674\n",
    "# PrefixOnly: 0\n",
    "# Empty:1109\n",
    "\n",
    "# Total: 5876\n",
    "# Valid : 5528\n",
    "# PrefixOnly: 0\n",
    "# Empty:348\n",
    "    \n",
    "# Total: 6237\n",
    "# Valid : 5865\n",
    "# PrefixOnly: 0\n",
    "# Empty:372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"D·ª±a tr√™n c√°c b√¨nh lu·∫≠n tr√™n m·∫°ng x√£ h·ªôi v·ªÅ Kh√°ch S·∫°n TTC Hotel Premium Ng·ªçc Lan, ta c√≥ th·ªÉ t·ªïng h·ª£p c·∫£m x√∫c chung nh∆∞ sau: Ph√≤ng ·ªëc: Ph√≤ng th√¨ ƒë∆∞·ª£c nh·∫≠n x√©t l√† r·ªông r√£i, s·∫°ch s·∫Ω, v√† v·ªá sinh t·ªët. M·ªôt s·ªë ng∆∞·ªùi c·∫£m th·∫•y ph√≤ng ·ªëc kh√° s·∫°ch s·∫Ω v√† t∆∞∆°m t·∫•t, v·ªõi trang thi·∫øt b·ªã ƒë·∫ßy ƒë·ªß. T·ªïng th·ªÉ, ƒëa s·ªë c·∫£m th·∫•y h√†i l√≤ng v·ªõi ch·∫•t l∆∞·ª£ng ph√≤ng ·ªëc. D·ªãch v·ª• v√† Nh√¢n vi√™n: Nh√¢n vi√™n ƒë∆∞·ª£c ƒë√°nh gi√° l√† l·ªãch s·ª± khi ƒë√≥n ti·∫øp v√† ph·ª•c v·ª• kh√°ch h√†ng t·ªët. Th√°i ƒë·ªô ph·ª•c v·ª• c·ªßa nh√¢n vi√™n ƒë∆∞·ª£c ƒë√°nh gi√° cao, c√≥ m·ª©c ƒë·ªô th√¢n thi·ªán v√† t·ªët. M·ªôt s·ªë b√¨nh lu·∫≠n nh·∫•n m·∫°nh r·∫±ng l·ªÖ t√¢n kh√¥ng th√¢n thi·ªán l·∫Øm, nh∆∞ng nh·ªØng m·∫∑t kh√°c c·ªßa d·ªãch v·ª• v√† nh√¢n vi√™n ƒë∆∞·ª£c ƒë√°nh gi√° t√≠ch c·ª±c. Ch·∫•t l∆∞·ª£ng d·ªãch v·ª•: ƒêa s·ªë c·∫£m th·∫•y ch·∫•t l∆∞·ª£ng d·ªãch v·ª• ƒë∆∞a ra kh√° ·ªïn, v·ªõi m·ªôt s·ªë b√¨nh lu·∫≠n ch·ªâ tr√≠ch v·ªÅ l·ªÖ t√¢n kh√¥ng th√¢n thi·ªán l·∫Øm. T·ªïng Th·ªÉ: Nhi·ªÅu ng∆∞·ªùi b√†y t·ªè s·ª± h√†i l√≤ng t·ªïng th·ªÉ v·ªõi kh√°ch s·∫°n, v√† m·ªôt s·ªë ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác h·ªç r·∫•t h√†i l√≤ng khi ·ªü ƒë√≥. C√≥ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng l√† 4 sao v√† c·∫£m nh·∫≠n r·∫±ng kh√°ch s·∫°n ƒë√£ ƒë√°p ·ª©ng ƒë·∫ßy ƒë·ªß c√°c d·ªãch v·ª•. T√≥m l·∫°i, d√π c√≥ m·ªôt s·ªë √Ω ki·∫øn ti√™u c·ª±c v·ªÅ th√°i ƒë·ªô c·ªßa l·ªÖ t√¢n, ƒëa s·ªë ng∆∞·ªùi d√πng v·∫´n c·∫£m th·∫•y h√†i l√≤ng v·ªõi Kh√°ch S·∫°n TTC Hotel Premium Ng·ªçc Lan, ƒë·∫∑c bi·ªát l√† v·ªõi c√°c kh√≠a c·∫°nh nh∆∞ ph√≤ng ·ªëc v√† d·ªãch v·ª• c·ªßa nh√¢n vi√™n.\"\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"{\"output\":\"D·ª±a tr√™n c√°c b√¨nh lu·∫≠n tr√™n m·∫°ng x√£ h·ªôi v·ªÅ Kh√°ch S·∫°n TTC Hotel Premium Ng·ªçc Lan, ta c√≥ th·ªÉ t·ªïng h·ª£p c·∫£m x√∫c chung nh∆∞ sau: Ph√≤ng ·ªëc: Ph√≤ng th√¨ ƒë∆∞·ª£c nh·∫≠n x√©t l√† r·ªông r√£i, s·∫°ch s·∫Ω, v√† v·ªá sinh t·ªët. M·ªôt s·ªë ng∆∞·ªùi c·∫£m th·∫•y ph√≤ng ·ªëc kh√° s·∫°ch s·∫Ω v√† t∆∞∆°m t·∫•t, v·ªõi trang thi·∫øt b·ªã ƒë·∫ßy ƒë·ªß. T·ªïng th·ªÉ, ƒëa s·ªë c·∫£m th·∫•y h√†i l√≤ng v·ªõi ch·∫•t l∆∞·ª£ng ph√≤ng ·ªëc. D·ªãch v·ª• v√† Nh√¢n vi√™n: Nh√¢n vi√™n ƒë∆∞·ª£c ƒë√°nh gi√° l√† l·ªãch s·ª± khi ƒë√≥n ti·∫øp v√† ph·ª•c v·ª• kh√°ch h√†ng t·ªët. Th√°i ƒë·ªô ph·ª•c v·ª• c·ªßa nh√¢n vi√™n ƒë∆∞·ª£c ƒë√°nh gi√° cao, c√≥ m·ª©c ƒë·ªô th√¢n thi·ªán v√† t·ªët. M·ªôt s·ªë b√¨nh lu·∫≠n nh·∫•n m·∫°nh r·∫±ng l·ªÖ t√¢n kh√¥ng th√¢n thi·ªán l·∫Øm, nh∆∞ng nh·ªØng m·∫∑t kh√°c c·ªßa d·ªãch v·ª• v√† nh√¢n vi√™n ƒë∆∞·ª£c ƒë√°nh gi√° t√≠ch c·ª±c. Ch·∫•t l∆∞·ª£ng d·ªãch v·ª•: ƒêa s·ªë c·∫£m th·∫•y ch·∫•t l∆∞·ª£ng d·ªãch v·ª• ƒë∆∞a ra kh√° ·ªïn, v·ªõi m·ªôt s·ªë b√¨nh lu·∫≠n ch·ªâ tr√≠ch v·ªÅ l·ªÖ t√¢n kh√¥ng th√¢n thi·ªán l·∫Øm. T·ªïng Th·ªÉ: Nhi·ªÅu ng∆∞·ªùi b√†y t·ªè s·ª± h√†i l√≤ng t·ªïng th·ªÉ v·ªõi kh√°ch s·∫°n, v√† m·ªôt s·ªë ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác h·ªç r·∫•t h√†i l√≤ng khi ·ªü ƒë√≥. C√≥ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng l√† 4 sao v√† c·∫£m nh·∫≠n r·∫±ng kh√°ch s·∫°n ƒë√£ ƒë√°p ·ª©ng ƒë·∫ßy ƒë·ªß c√°c d·ªãch v·ª•. T√≥m l·∫°i, d√π c√≥ m·ªôt s·ªë √Ω ki·∫øn ti√™u c·ª±c v·ªÅ th√°i ƒë·ªô c·ªßa l·ªÖ t√¢n, ƒëa s·ªë ng∆∞·ªùi d√πng v·∫´n c·∫£m th·∫•y h√†i l√≤ng v·ªõi Kh√°ch S·∫°n TTC Hotel Premium Ng·ªçc Lan, ƒë·∫∑c bi·ªát l√† v·ªõi c√°c kh√≠a c·∫°nh nh∆∞ ph√≤ng ·ªëc v√† d·ªãch v·ª• c·ªßa nh√¢n vi√™n.\"} \"\"\"[10:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 0\n",
      "Valid : 0\n",
      "PrefixOnly: 0\n",
      "Empty:0\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 1500:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = json.load(f)\n",
    "                            isPrefixOnly_files = False\n",
    "                            for i in search_string:\n",
    "                                if i in content['output']:\n",
    "#                             if content.count(search_string) == 5:\n",
    "                                    prefixOnly_files.append(file_path)\n",
    "                                    isPrefixOnly_files = True\n",
    "                            if not isPrefixOnly_files:\n",
    "                                valid_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_to_search = 'instruction_social_data/240102_instructions_social_aspectSA_6k7_idx0_thuc_prompt_v3'\n",
    "search_string = ['network error', 'Message', 'reached our limit of messages per hour. Please try again later', 'All browsers are currently busy', 'An error occurred', '/home/thuc-ubuntu']\n",
    "\n",
    "result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "\n",
    "print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "Valid : {len(result_dict['valid']) }\n",
    "PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for file in result_dict['valid']:\n",
    "    dest = file.replace('240102_instructions_social_aspectSA_6k7_idx0_thuc_prompt_v3', '240102_instructions_social_aspectSA_5k2_idx0_thuc_prompt_v3')\n",
    "#     print(file)\n",
    "#     print(dest)\n",
    "    shutil.copy(file,dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIKI reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 6713\n",
      "Valid : 6308\n",
      "PrefixOnly: 6\n",
      "Empty:399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json\n",
    "\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 1500:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = json.load(f)\n",
    "                        isPrefixOnly_files = False\n",
    "#                             print(file, content['output'][:10]\n",
    "                        for i in search_string:\n",
    "                            if i in content['output']:\n",
    "#                             if content.count(search_string) == 5:\n",
    "                                prefixOnly_files.append(file_path)\n",
    "                                isPrefixOnly_files = True\n",
    "                        if not isPrefixOnly_files:\n",
    "                            valid_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_to_search = 'instruction_social_data/240102_instructions_social_tikiReview_idx0_6k7_thuc_prompt_v3'\n",
    "search_string = ['network error', 'Message', 'reached our limit of messages per hour. Please try again later', 'All browsers are currently busy', 'An error occurred', '/home/thuc-ubuntu']\n",
    "\n",
    "result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "\n",
    "print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "Valid : {len(result_dict['valid']) }\n",
    "PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for file in result_dict['valid']:\n",
    "    dest = file.replace('240102_instructions_social_tikiReview_idx0_6k7_thuc_prompt_v3', '240102_instructions_social_tikiReview_idx0_6k3_thuc_prompt_v3')\n",
    "#     print(file)\n",
    "#     print(dest)\n",
    "    shutil.copy(file,dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([{ \"name\": \"frunzepastrr\", \"email\": \"frunzepastrr@hotmail.com\", \"passw\": \"MgvQHOdP83\", \"cookies\": \"/Cookies/frunzepastrr@hotmail.com.json\" },\n",
    "{ \"name\": \"jankimjennyt9txrkgm1l0eaqgk\", \"email\": \"jankimjennyt9txrkgm1l0eaqgk@hotmail.com\", \"passw\": \"Eegjgh268e\", \"cookies\": \"/Cookies/jankimjennyt9txrkgm1l0eaqgk@hotmail.com.json\" },\n",
    "{ \"name\": \"arrowarnaoh\", \"email\": \"arrowarnaoh@hotmail.com\", \"passw\": \"KgWbEZaG38\", \"cookies\": \"/Cookies/arrowarnaoh@hotmail.com.json\" },\n",
    "{ \"name\": \"velmanbhumak0\", \"email\": \"velmanbhumak0@hotmail.com\", \"passw\": \"xyD0Zcp854\", \"cookies\": \"/Cookies/velmanbhumak0@hotmail.com.json\" },\n",
    "{ \"name\": \"relayonnonac\", \"email\": \"relayonnonac@hotmail.com\", \"passw\": \"0O61sHsH92\", \"cookies\": \"/Cookies/relayonnonac@hotmail.com.json\" },\n",
    "{ \"name\": \"Nova001\", \"email\": \"novachatbot001@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/novachatbot001@gmail.com.json\" },\n",
    "{ \"name\": \"Nova002\", \"email\": \"novachatbot002@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/novachatbot002@gmail.com.json\" },\n",
    "{ \"name\": \"Nova004\", \"email\": \"novachatbot004@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/novachatbot004@gmail.com.json\" },\n",
    "{ \"name\": \"khangaia112233\", \"email\": \"khangaia112233@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/khangaia112233@gmail.com.json\" },\n",
    "{ \"name\": \"khangaia223344\", \"email\": \"khangaia223344@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/khangaia223344@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam079\", \"email\": \"aiateam079@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam079@gmail.com.json\" }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ \"name\": \"aiateam278\", \"email\": \"aiateam278@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam278@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam278123\", \"email\": \"aiateam278123@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam278123@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam0431\", \"email\": \"aiateam0431@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam0431@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam648one\", \"email\": \"aiateam648one@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam648one@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam043\", \"email\": \"aiateam043@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam043@gmail.com.json\" },\n",
    "{ \"name\": \"aiateam236\", \"email\": \"aiateam236@gmail.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam236@gmail.com.json\" },\n",
    "{ \"name\": \"outlook0001\", \"email\": \"aiateam0001@outlook.com\", \"passw\": \"aia_@123\", \"cookies\": \"/Cookies/aiateam0001@outlook.com.json\" },\n",
    "{ \"name\": \"dilmanisaquei\", \"email\": \"dilmanisaquei@hotmail.com\", \"passw\": \"CJ455Wu628\", \"cookies\": \"/Cookies/dilmanisaquei@hotmail.com.json\" },\n",
    "{\"name\": \"Zawed\", \"email\": \"zawednzekwe2@hotmail.com\", \"passw\": \"X6Iocz2O93\", \"cookies\": \"zawednzekwe2@hotmail.com.json\"},\n",
    "{\"name\": \"Moyun\", \"email\": \"moyunlusuphs@hotmail.com\", \"passw\": \"UIHYlphy15\", \"cookies\": \"moyunlusuphs@hotmail.com.json\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"name\": \"Gf\", \"email\": \"gfkblankoz@hotmail.com\", \"passw\": \"J2erkGYG50\", \"cookies\": \"gfkblankoz@hotmail.com.json\"},\n",
    "{\"name\": \"Waart\", \"email\": \"waartniandu@hotmail.com\", \"passw\": \"gCcRszgC73\", \"cookies\": \"waartniandu@hotmail.com.json\"},\n",
    "{\"name\": \"Yagah\", \"email\": \"yagahuertask@hotmail.com\", \"passw\": \"6FhX0UVp21\", \"cookies\": \"yagahuertask@hotmail.com.json\"},\n",
    "{\"name\": \"Wougu\", \"email\": \"wouguemidurae@hotmail.com\", \"passw\": \"s5pmSZOy20\", \"cookies\": \"wouguemidurae@hotmail.com.json\"},\n",
    "{\"name\": \"Hamaz\", \"email\": \"hamazmutemar@hotmail.com\", \"passw\": \"qVXUR0pe41\", \"cookies\": \"hamazmutemar@hotmail.com.json\"},\n",
    "{\"name\": \"quoby\", \"email\": \"quobythaml@hotmail.com\", \"passw\": \"E8xJJssF95\", \"cookies\": \"quobythaml@hotmail.com.json\"},\n",
    "{\"name\": \"Sarus\", \"email\": \"saruselsadap@hotmail.com\", \"passw\": \"e79nVVMO84\", \"cookies\": \"/Cookies/saruselsadap@hotmail.com.json\"},\n",
    "{\"name\": \"Genso\", \"email\": \"gensonhrethw@hotmail.com\", \"passw\": \"UCg1VMWr63\", \"cookies\": \"/Cookies/gensonhrethw@hotmail.com.json\"},\n",
    "{\"name\": \"Dobra\", \"email\": \"dobraiyumi@hotmail.com\", \"passw\": \"6Z8Z81gK24\", \"cookies\": \"/Cookies/dobraiyumi@hotmail.com.json\"},\n",
    "{\"name\": \"Lijp\", \"email\": \"lijpealdw0@hotmail.com\", \"passw\": \"GAiSkQO734\", \"cookies\": \"/Cookies/lijpealdw0@hotmail.com.json\"},\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"name\": \"Ninno\", \"email\": \"ninnorakowak@hotmail.com\", \"passw\": \"yvJ4qloH34\", \"cookies\": \"/Cookies/ninnorakowak@hotmail.com.json\"},\n",
    "{\"name\": \"Timmu\", \"email\": \"timmuquhilap@hotmail.com\", \"passw\": \"n8y1Gb5N17\", \"cookies\": \"/Cookies/timmuquhilap@hotmail.com.json\"},\n",
    "{\"name\": \"Deopa\", \"email\": \"deopamohmodr@hotmail.com\", \"passw\": \"dVMPpCVa70\", \"cookies\": \"/Cookies/deopamohmodr@hotmail.com.json\"},\n",
    "{\"name\": \"Stair\", \"email\": \"stairsgeelyx@hotmail.com\", \"passw\": \"z0gdEceI62\", \"cookies\": \"/Cookies/stairsgeelyx@hotmail.com.json\"},\n",
    "{\"name\": \"Embor\", \"email\": \"emborginbergi@hotmail.com\", \"passw\": \"Zel4SbyQ64\", \"cookies\": \"/Cookies/emborginbergi@hotmail.com.json\"},\n",
    "{\"name\": \"Okun\", \"email\": \"okunmatatib@hotmail.com\", \"passw\": \"VM75kb8C68\", \"cookies\": \"/Cookies/okunmatatib@hotmail.com.json\"},\n",
    "{\"name\": \"Grindy\", \"email\": \"grindysheukin@hotmail.com\", \"passw\": \"eZ6kDItI16\", \"cookies\": \"/Cookies/grindysheukin@hotmail.com.json\"},\n",
    "{\"name\": \"Anuma\", \"email\": \"anumaatunahg@hotmail.com\", \"passw\": \"mMtklxCU98\", \"cookies\": \"/Cookies/anumaatunahg@hotmail.com.json\"},\n",
    "{\"name\": \"Nysh\", \"email\": \"nyshdimpo2@hotmail.com\", \"passw\": \"Gvmvd4IN92\", \"cookies\": \"/Cookies/nyshdimpo2@hotmail.com.json\"},\n",
    "{\"name\": \"Lina\", \"email\": \"linakoqetelom@hotmail.com\", \"passw\": \"hXNgiDu476\", \"cookies\": \"linakoqetelom@hotmail.com.json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35774.json: Expecting value: line 34 column 17 (char 3251)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38185.json: Expecting value: line 59 column 36 (char 4719)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35352.json: Expecting value: line 85 column 17 (char 5325)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/42783.json: Expecting value: line 9 column 21 (char 2161)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/43745.json: Expecting value: line 37 column 28 (char 3307)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/41338.json: Expecting value: line 15 column 17 (char 3084)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/42346.json: Expecting value: line 19 column 51 (char 2816)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/42909.json: Expecting value: line 17 column 41 (char 2973)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38752.json: Expecting value: line 50 column 17 (char 4399)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/39940.json: Expecting value: line 42 column 17 (char 3503)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35530.json: Expecting value: line 43 column 17 (char 3724)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/39813.json: Expecting value: line 55 column 40 (char 4548)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/37621.json: Expecting value: line 63 column 31 (char 4724)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/37694.json: Expecting value: line 12 column 17 (char 2991)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/41368.json: Expecting value: line 92 column 62 (char 5924)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/45432.json: Expecting value: line 54 column 21 (char 4181)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38280.json: Expecting value: line 12 column 17 (char 2434)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38389.json: Expecting value: line 8 column 17 (char 2334)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38248.json: Expecting value: line 71 column 17 (char 4917)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/39794.json: Expecting value: line 23 column 17 (char 3117)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/42676.json: Expecting value: line 31 column 41 (char 3956)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/43275.json: Expecting value: line 21 column 67 (char 2835)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/37178.json: Expecting value: line 33 column 17 (char 3206)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/36801.json: Expecting value: line 30 column 17 (char 2970)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35471.json: Expecting value: line 108 column 17 (char 5809)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/39256.json: Expecting value: line 24 column 21 (char 2900)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35883.json: Expecting value: line 23 column 36 (char 3121)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/40548.json: Expecting value: line 16 column 17 (char 2934)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/36266.json: Expecting value: line 20 column 17 (char 2608)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/40019.json: Expecting value: line 41 column 17 (char 3704)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35835.json: Expecting value: line 20 column 17 (char 3051)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/38278.json: Expecting value: line 30 column 21 (char 3362)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/36471.json: Expecting value: line 33 column 17 (char 3154)\n",
      "Error reading file instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/44044.json: Expecting value: line 19 column 41 (char 2725)\n",
      "Total: 9518\n",
      "Valid : 9003\n",
      "PrefixOnly: 0\n",
      "Empty:515\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "lst = []\n",
    "def get_files_with_string(directory, search_string):\n",
    "    valid_files = []\n",
    "    empty_files = []\n",
    "    prefixOnly_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_bytes = os.path.getsize(file_path)\n",
    "                if size_bytes <= 3000:\n",
    "                    empty_files.append(file_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = json.load(f)\n",
    "                        lst.append(len(content['output']))\n",
    "                        if len(str(content['output'])) > 1000 and len(str(content['output'])) < 3000 and len(content['output']) > 4 and len(content['output']) < 10:\n",
    "                            isPrefixOnly_files = False\n",
    "                            for i in search_string:\n",
    "                                if i in content['output']:\n",
    "    #                             if content.count(search_string) == 5:\n",
    "                                    prefixOnly_files.append(file_path)\n",
    "                                    isPrefixOnly_files = True\n",
    "                            if not isPrefixOnly_files:\n",
    "                                valid_files.append(file_path)\n",
    "                        else:\n",
    "                            empty_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    return {\"valid\": valid_files, \"prefixOnly\": prefixOnly_files, \"empty\": empty_files}\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "directory_to_search = 'instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6'\n",
    "search_string = ['network error', 'Message', 'reached our limit of messages per hour. Please try again later', 'All browsers are currently busy', 'An error occurred', '/home/thuc-ubuntu']\n",
    "\n",
    "result_dict = get_files_with_string(directory_to_search, search_string)\n",
    "\n",
    "print(f\"\"\"Total: {sum(map(len, result_dict.values()))}\n",
    "Valid : {len(result_dict['valid']) }\n",
    "PrefixOnly: {len(result_dict['prefixOnly'])}\n",
    "Empty:{len(result_dict['empty'])}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 4, 2: 80, 3: 39, 4: 86, 5: 5840, 6: 501, 7: 359, 8: 2583, 9: 4, 11: 2}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct = {i:0 for i in set(lst)}\n",
    "for i in lst:\n",
    "    dct[i] +=1\n",
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/40006.json\n",
      "4692\n",
      "5\n",
      "989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'k·ªπ s∆∞ AI h·ªèi': 'Ph√¢n lo·∫°i danh m·ª•c ng√†nh h√†ng v√† t√™n s·∫£n ph·∫©m (n·∫øu c√≥) t·ª´ b√†i vi·∫øt',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': {'danh_m·ª•c_ng√†nh_h√†ng': '·∫®m th·ª±c',\n",
       "   't√™n_s·∫£n_ph·∫©m': ['Th·ªãt b√≤ x√†o s√∫p l∆°']}},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'T√≥m t·∫Øt c·∫£m x√∫c n·ªïi b·∫≠t t·ª´ c√°c b√¨nh lu·∫≠n v·ªÅ b√†i vi·∫øt',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': {'t√≥m_t·∫Øt_c·∫£m_x√∫c': ['Th√≠ch th√∫', 'H·ª©ng th√∫']}},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'T√≥m t·∫Øt √Ω ƒë·ªì n·ªïi b·∫≠t c·ªßa c√°c b√¨nh lu·∫≠n',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': {'√Ω_ƒë·ªì_n·ªïi_b·∫≠t': ['ƒê·ªÅ xu·∫•t mua rau t·ª´ shop',\n",
       "    'Chia s·∫ª quy·∫øt ƒë·ªãnh t·ª± n·∫•u m√≥n ƒÉn']}},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'ƒê·ªÅ xu·∫•t c√°c emoji ph√π h·ª£p n·ªôi dung b√†i vi·∫øt, b√¨nh lu·∫≠n k√®m gi·∫£i th√≠ch',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': {'emoji_g·ª£i_√Ω': [{'emoji': 'üç≤',\n",
       "     'gi·∫£i_th√≠ch': 'Li√™n quan ƒë·∫øn ·∫©m th·ª±c'},\n",
       "    {'emoji': 'ü•©', 'gi·∫£i_th√≠ch': 'Th·ªÉ hi·ªán th·ªãt b√≤'},\n",
       "    {'emoji': 'üåø', 'gi·∫£i_th√≠ch': 'T∆∞·ª£ng tr∆∞ng cho s√∫p l∆°'}]}},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'B√†i vi·∫øt h∆∞·ªõng ƒë·∫øn nh√≥m ƒë·ªëi t∆∞·ª£ng ng∆∞·ªùi d√πng n√†o?',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': 'B√†i vi·∫øt h∆∞·ªõng ƒë·∫øn nh√≥m ng∆∞·ªùi y√™u th√≠ch ·∫©m th·ª±c v√† mu·ªën t·ª± n·∫•u ƒÉn t·∫°i nh√†.'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "list_file = result_dict['empty']\n",
    "# i  = random.randint(3000, len(list_file))\n",
    "file_path = random.choice(list_file)\n",
    "with open(file_path, 'r',  encoding='utf-8') as f:\n",
    "    content = json.load(f)\n",
    "    f.close()\n",
    "print(file_path)\n",
    "print(os.path.getsize(file_path))\n",
    "print(len(content['output']))\n",
    "print(len(str(content['output'])))\n",
    "content['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction_social_data/240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6/35049.json\n",
      "6268\n",
      "8\n",
      "1840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'k·ªπ s∆∞ AI h·ªèi': 'Ph√¢n lo·∫°i danh m·ª•c ng√†nh h√†ng v√† t√™n s·∫£n ph·∫©m t·ª´ b√†i vi·∫øt',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': {'danh m·ª•c ng√†nh h√†ng': 'Th·ªùi trang',\n",
       "   't√™n s·∫£n ph·∫©m': ['Kho√°c', 'Cv', '√Åo len']}},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'T·∫°o m·ªôt v√†i b√¨nh lu·∫≠n m·ªõi ph·∫£n h·ªìi b√†i vi·∫øt m·ªôt c√°ch t·ª± nhi√™n v√† PH·∫¢I tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng LIST',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': ['Outfit m√πa thu n√†y th·∫≠t s·ª± xinh ƒë·∫πp!',\n",
       "   'M√¨nh r·∫•t th√≠ch ki·ªÉu kho√°c 211001001, c√≥ n√™n ƒë·∫∑t ngay kh√¥ng nh·ªâ?',\n",
       "   '∆Øu ƒë√£i freeship v√† voucher 10% l√† c∆° h·ªôi t·ªët!']},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'T√≥m t·∫Øt √Ω ƒë·ªì c·ªßa c√°c b√¨nh lu·∫≠n',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': 'C√°c b√¨nh lu·∫≠n ch·ªß y·∫øu l√† ng∆∞·ªùi quan t√¢m v√† mu·ªën ƒë·∫∑t mua s·∫£n ph·∫©m, ƒë√°nh gi√° t√≠ch c·ª±c v·ªÅ thi·∫øt k·∫ø v√† ∆∞u ƒë√£i.'},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'Ph√¢n t√≠ch th√¥ng tin th·ª±c th·ªÉ chi ti·∫øt v√† PH·∫¢I tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng JSON chuy√™n nghi·ªáp',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': {'S·∫£n ph·∫©m': ['Kho√°c (211001001)',\n",
       "    'Cv (210921001)',\n",
       "    '√Åo len (210925111)'],\n",
       "   '∆Øu ƒë√£i': ['Freeship khi mua nguy√™n set', 'Voucher 10% trong th√°ng 10'],\n",
       "   'D·ªãch v·ª•': ['Ship COD to√†n qu·ªëc', 'Ki·ªÉm h√†ng tr∆∞·ªõc thanh to√°n']}},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'B√†i vi·∫øt h∆∞·ªõng ƒë·∫øn nh√≥m ƒë·ªëi t∆∞·ª£ng ng∆∞·ªùi d√πng n√†o?',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': 'B√†i vi·∫øt h∆∞·ªõng ƒë·∫øn nh√≥m ng∆∞·ªùi y√™u th·ªùi trang, ƒë·∫∑c bi·ªát l√† ph·ª• n·ªØ quan t√¢m ƒë·∫øn outfit thu ƒë√¥ng thanh l·ªãch v√† nh·∫π nh√†ng.'},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'T·∫°o ra m·ªôt b·∫£n t√≥m t·∫Øt n·ªôi dung b√†i vi·∫øt',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': 'B√†i vi·∫øt gi·ªõi thi·ªáu outfit thu ƒë√¥ng, v·ªõi c√°c s·∫£n ph·∫©m nh∆∞ kho√°c, cv, √°o len. Freeship v√† voucher 10% l√† ∆∞u ƒë√£i ƒë·∫∑c bi·ªát.'},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'T√≥m t·∫Øt c·∫£m x√∫c ƒëa s·ªë c√°c b√¨nh lu·∫≠n v·ªÅ b√†i vi·∫øt',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': 'ƒêa s·ªë c·∫£m x√∫c t√≠ch c·ª±c, ng∆∞·ªùi ƒë·ªçc ƒë√°nh gi√° cao v·ªÅ s·∫£n ph·∫©m v√† ch·∫•t l∆∞·ª£ng ph·ª•c v·ª•.'},\n",
       " {'k·ªπ s∆∞ AI h·ªèi': 'ƒê·ªÅ xu·∫•t c√°c emoji ph√π h·ª£p n·ªôi dung b√†i vi·∫øt, b√¨nh lu·∫≠n k√®m gi·∫£i th√≠ch',\n",
       "  'k·∫øt qu·∫£ t·ª´ tr·ª£ l√Ω AI NLP': {'emoji b√†i vi·∫øt': ['‚ú®', 'üëâ', 'üìå', '‚ù§Ô∏è'],\n",
       "   'emoji b√¨nh lu·∫≠n': ['‚ù§Ô∏è', 'üëç', 'ü§©']}}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "list_file = result_dict['valid']\n",
    "file_path = random.choice(list_file)\n",
    "with open(file_path, 'r',  encoding='utf-8') as f:\n",
    "    content = json.load(f)\n",
    "    f.close()\n",
    "print(file_path)\n",
    "print(os.path.getsize(file_path))\n",
    "print(len(content['output']))\n",
    "print(len(str(content['output'])))\n",
    "content['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for file in result_dict['valid']:\n",
    "    dest = file.replace('240108_instructions_social_multiTurnChat_Post111k_idx34000_thuc_prompt_v6', '240109_instructions_social_multiTurnChat_Post111k_idx34000_9k_thuc_prompt_v6')\n",
    "#     print(file)\n",
    "#     print(dest)\n",
    "    shutil.copy(file,dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
